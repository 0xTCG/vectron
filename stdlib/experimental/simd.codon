@tuple
class Vec[T, N: Static[int]]:
    ZERO_16x8i = Vec[u8,16](u8(0))
    FF_16x8i = Vec[u8,16](u8(0xff))
    ZERO_32x8i = Vec[u8,32](u8(0))
    FF_32x8i = Vec[u8,32](u8(0xff))
    
    @llvm
    def _mm_set1_epi8(val: u8) -> Vec[u8, 16]:
        %0 = insertelement <16 x i8> undef, i8 %val, i32 0
        %1 = shufflevector <16 x i8> %0, <16 x i8> undef, <16 x i32> zeroinitializer
        ret <16 x i8> %1

    @llvm
    def _mm64_set1_epi8(val: u8) -> Vec[u8, 8]:
        %0 = insertelement <8 x i8> undef, i8 %val, i32 0
        %1 = shufflevector <8 x i8> %0, <8 x i8> undef, <8 x i32> zeroinitializer
        ret <8 x i8> %1        

    @llvm
    def _mm256_set1_epi8(val: u8) -> Vec[u8, 32]:
        %0 = insertelement <32 x i8> undef, i8 %val, i32 0
        %1 = shufflevector <32 x i8> %0, <32 x i8> undef, <32 x i32> zeroinitializer
        ret <32 x i8> %1

    @llvm
    def _mm512_set1_epi8(val: u8) -> Vec[u8, 64]:
        %0 = insertelement <64 x i8> undef, i8 %val, i32 0
        %1 = shufflevector <64 x i8> %0, <64 x i8> undef, <64 x i32> zeroinitializer
        ret <64 x i8> %1        

    @llvm
    def _mm_loadu_si128(data) -> Vec[u8, 16]:
        %0 = bitcast i8* %data to <16 x i8>*
        %1 = load <16 x i8>, <16 x i8>* %0, align 1
        ret <16 x i8> %1

    @llvm
    def _mm_loadu_si64(data) -> Vec[u8, 8]:
        %0 = bitcast i8* %data to <8 x i8>*
        %1 = load <8 x i8>, <8 x i8>* %0, align 1
        ret <8 x i8> %1        

    @llvm
    def _mm256_loadu_si256(data) -> Vec[u8, 32]:
        %0 = bitcast i8* %data to <32 x i8>*
        %1 = load <32 x i8>, <32 x i8>* %0, align 1
        ret <32 x i8> %1

    @llvm
    def _mm512_loadu_si512(data) -> Vec[u8, 64]:
        %0 = bitcast i8* %data to <64 x i8>*
        %1 = load <64 x i8>, <64 x i8>* %0, align 1
        ret <64 x i8> %1        

    @llvm
    def _mm256_set1_ps(val: f32) -> Vec[f32, 8]:
        %0 = insertelement <8 x float> undef, float %val, i32 0
        %1 = shufflevector <8 x float> %0, <8 x float> undef, <8 x i32> zeroinitializer
        ret <8 x float> %1

    @llvm
    def _mm512_set1_ps(val: f32) -> Vec[f32, 16]:
        %0 = insertelement <16 x float> undef, float %val, i32 0
        %1 = shufflevector <16 x float> %0, <16 x float> undef, <16 x i32> zeroinitializer
        ret <16 x float> %1

    @llvm
    def _mm512_set1_epi32(val: i32) -> Vec[i32, 16]:
        %0 = insertelement <16 x i32> undef, i32 %val, i32 0
        %1 = shufflevector <16 x i32> %0, <16 x i32> undef, <16 x i32> zeroinitializer
        ret <16 x i32> %1

    @llvm
    def _mm256_set1_epi16(val: i16) -> Vec[i16, 16]:
        %0 = insertelement <16 x i16> undef, i16 %val, i16 0
        %1 = shufflevector <16 x i16> %0, <16 x i16> undef, <16 x i32> zeroinitializer
        ret <16 x i16> %1

    @llvm
    def _mm128_set1_epi16(val: i16) -> Vec[i16, 8]:
        %0 = insertelement <8 x i16> undef, i16 %val, i16 0
        %1 = shufflevector <8 x i16> %0, <8 x i16> undef, <8 x i32> zeroinitializer
        ret <8 x i16> %1        

    @llvm
    def _mm1024_set1_epi16(val: i16) -> Vec[i16, 64]:
        %0 = insertelement <64 x i16> undef, i16 %val, i16 0
        %1 = shufflevector <64 x i16> %0, <64 x i16> undef, <64 x i32> zeroinitializer
        ret <64 x i16> %1

    @llvm
    def _mm512_set1_epi16(val: i16) -> Vec[i16, 32]:
        %0 = insertelement <32 x i16> undef, i16 %val, i16 0
        %1 = shufflevector <32 x i16> %0, <32 x i16> undef, <32 x i32> zeroinitializer
        ret <32 x i16> %1        

    @llvm
    def _mm256_set1_epi32(val: i32) -> Vec[i32, 8]:
        %0 = insertelement <8 x i32> undef, i32 %val, i32 0
        %1 = shufflevector <8 x i32> %0, <8 x i32> undef, <8 x i32> zeroinitializer
        ret <8 x i32> %1

    @llvm
    def _mm128_set1_epi32(val: i32) -> Vec[i32, 4]:
        %0 = insertelement <4 x i32> undef, i32 %val, i32 0
        %1 = shufflevector <4 x i32> %0, <4 x i32> undef, <4 x i32> zeroinitializer
        ret <4 x i32> %1

    @llvm
    def _mm256_loadu_ps(data: Ptr[f32]) -> Vec[f32, 8]:
        %0 = bitcast float* %data to <8 x float>*
        %1 = load <8 x float>, <8 x float>* %0
        ret <8 x float> %1     

    @llvm
    def _mm512_loadu_ps(data: Ptr[f32]) -> Vec[f32, 16]:
        %0 = bitcast float* %data to <16 x float>*
        %1 = load <16 x float>, <16 x float>* %0
        ret <16 x float> %1

    @llvm
    def _mm512_loadu_epi32(data: Ptr[i32]) -> Vec[i32, 16]:
        %0 = bitcast i32* %data to <16 x i32>*
        %1 = load <16 x i32>, <16 x i32>* %0
        ret <16 x i32> %1

    @llvm
    def _mm256_loadu_epi16(data: Ptr[i16]) -> Vec[i16, 16]:
        %0 = bitcast i16* %data to <16 x i16>*
        %1 = load <16 x i16>, <16 x i16>* %0
        ret <16 x i16> %1

    @llvm
    def _mm128_loadu_epi16(data: Ptr[i16]) -> Vec[i16, 8]:
        %0 = bitcast i16* %data to <8 x i16>*
        %1 = load <8 x i16>, <8 x i16>* %0
        ret <8 x i16> %1        

    @llvm
    def _mm1024_loadu_epi16(data: Ptr[i16]) -> Vec[i16, 64]:
        %0 = bitcast i16* %data to <64 x i16>*
        %1 = load <64 x i16>, <64 x i16>* %0
        ret <64 x i16> %1   

    @llvm
    def _mm512_loadu_epi16(data: Ptr[i16]) -> Vec[i16, 32]:
        %0 = bitcast i16* %data to <32 x i16>*
        %1 = load <32 x i16>, <32 x i16>* %0
        ret <32 x i16> %1                

    @llvm
    def _mm256_loadu_epi32(data: Ptr[i32]) -> Vec[i32, 8]:
        %0 = bitcast i32* %data to <8 x i32>*
        %1 = load <8 x i32>, <8 x i32>* %0
        ret <8 x i32> %1        

    @llvm
    def _mm128_loadu_epi32(data: Ptr[i32]) -> Vec[i32, 4]:
        %0 = bitcast i32* %data to <4 x i32>*
        %1 = load <4 x i32>, <4 x i32>* %0
        ret <4 x i32> %1

    @llvm
    def _mm256_cvtepi8_epi32(vec: Vec[u8, 16]) -> Vec[u32, 8]:
        %0 = shufflevector <16 x i8> %vec, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
        %1 = sext <8 x i8> %0 to <8 x i32>
        ret <8 x i32> %1

    @llvm
    def _mm512_cvtepi8_epi64(vec: Vec[u8, 32]) -> Vec[u32, 16]:
        %0 = shufflevector <32 x i8> %vec, <32 x i8> undef, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
        %1 = sext <16 x i8> %0 to <16 x i32>
        ret <16 x i32> %1

    @llvm
    def _mm256_castsi256_ps(vec: Vec[u32, 8]) -> Vec[f32, 8]:
        %0 = bitcast <8 x i32> %vec to <8 x float>
        ret <8 x float> %0   
          
    @llvm
    def _mm512_castsi512_ps(vec: Vec[u32, 16]) -> Vec[f32, 16]:
        %0 = bitcast <16 x i32> %vec to <16 x float>
        ret <16 x float> %0

    def __new__(x, T: type, N: Static[int]) -> Vec[T, N]:
        if isinstance(T, u8) and N == 8:
            if isinstance(x, u8) or isinstance(x, byte): # TODO: u8<->byte
                return Vec._mm64_set1_epi8(x)
            if isinstance(x, Ptr[u8]) or isinstance(x, Ptr[byte]):
                return Vec._mm_loadu_si64(x)
            if isinstance(x, str):
                return Vec._mm_loadu_si64(x.ptr)
            if isinstance(x, List[u8]):
                return Vec._mm_loadu_si64(x.arr.ptr)       
        if isinstance(T, u8) and N == 16:
            if isinstance(x, u8) or isinstance(x, byte): # TODO: u8<->byte
                return Vec._mm_set1_epi8(x)
            if isinstance(x, Ptr[u8]) or isinstance(x, Ptr[byte]):
                return Vec._mm_loadu_si128(x)
            if isinstance(x, str):
                return Vec._mm_loadu_si128(x.ptr)
            if isinstance(x, List[u8]):
                return Vec._mm_loadu_si128(x.arr.ptr)                
        if isinstance(T, u8) and N == 32:
            if isinstance(x, u8) or isinstance(x, byte): # TODO: u8<->byte
                return Vec._mm256_set1_epi8(x)
            if isinstance(x, Ptr[u8]) or isinstance(x, Ptr[byte]):
                return Vec._mm256_loadu_si256(x)
            if isinstance(x, str):
                return Vec._mm256_loadu_si256(x.ptr)
            if isinstance(x, List[u8]):
                return Vec._mm256_loadu_si256(x.arr.ptr)
            if isinstance(x, List[str]):
                return Vec._mm256_loadu_si256(x.arr.ptr)                
        if isinstance(T, u8) and N == 64:
            if isinstance(x, u8) or isinstance(x, byte): # TODO: u8<->byte
                return Vec._mm512_set1_epi8(x)
            if isinstance(x, Ptr[u8]) or isinstance(x, Ptr[byte]):
                return Vec._mm512_loadu_si512(x)
            if isinstance(x, str):
                return Vec._mm512_loadu_si512(x.ptr)            
        if isinstance(T, f32) and N == 8:
            if isinstance(x, f32):
                return Vec._mm256_set1_ps(x)
            if isinstance(x, Ptr[f32]):  # TODO: multi-elif does NOT work with statics [why?!]
                return Vec._mm256_loadu_ps(x)
            if isinstance(x, List[f32]):
                return Vec._mm256_loadu_ps(x.arr.ptr)
            if isinstance(x, Vec[u8, 16]):
                return Vec._mm256_castsi256_ps(Vec._mm256_cvtepi8_epi32(x))
        if isinstance(T, f32) and N == 16:
            if isinstance(x, f32):
                return Vec._mm512_set1_ps(x)
            if isinstance(x, Ptr[f32]):  # TODO: multi-elif does NOT work with statics [why?!]
                return Vec._mm512_loadu_ps(x)
            if isinstance(x, List[f32]):
                return Vec._mm512_loadu_ps(x.arr.ptr)
            if isinstance(x, Vec[u8, 32]):
                return Vec._mm512_castsi512_ps(Vec._mm512_cvtepi8_epi64(x))
        if isinstance(T, i32) and N == 16:
            if isinstance(x, i32):
                return Vec._mm512_set1_epi32(x)
            if isinstance(x, Ptr[i32]):  # TODO: multi-elif does NOT work with statics [why?!]
                return Vec._mm512_loadu_epi32(x)
            if isinstance(x, List[i32]):
                return Vec._mm512_loadu_epi32(x.arr.ptr)
        if isinstance(T, i16) and N == 64:
            if isinstance(x, i16):
                return Vec._mm1024_set1_epi16(x)                
            if isinstance(x, Ptr[i16]):  # TODO: multi-elif does NOT work with statics [why?!]
                return Vec._mm1024_loadu_epi16(x)
            if isinstance(x, List[i16]):
                return Vec._mm1024_loadu_epi16(x.arr.ptr)    
        if isinstance(T, i16) and N == 32:
            if isinstance(x, i16):
                return Vec._mm512_set1_epi16(x)                
            if isinstance(x, Ptr[i16]):  # TODO: multi-elif does NOT work with statics [why?!]
                return Vec._mm512_loadu_epi16(x)
            if isinstance(x, List[i16]):
                return Vec._mm512_loadu_epi16(x.arr.ptr)                                                
        if isinstance(T, i16) and N == 16:
            if isinstance(x, i16):
                return Vec._mm256_set1_epi16(x)
            if isinstance(x, Ptr[i16]):  # TODO: multi-elif does NOT work with statics [why?!]
                return Vec._mm256_loadu_epi16(x)
            if isinstance(x, List[i16]):
                return Vec._mm256_loadu_epi16(x.arr.ptr)                
        if isinstance(T, i16) and N == 8:
            if isinstance(x, i16):
                return Vec._mm128_set1_epi16(x)
            if isinstance(x, Ptr[i16]):  # TODO: multi-elif does NOT work with statics [why?!]
                return Vec._mm128_loadu_epi16(x)
            if isinstance(x, List[i16]):
                return Vec._mm128_loadu_epi16(x.arr.ptr)                                
        if isinstance(T, i32) and N == 8:
            if isinstance(x, i32):
                return Vec._mm256_set1_epi32(x)
            if isinstance(x, Ptr[i32]):  # TODO: multi-elif does NOT work with statics [why?!]
                return Vec._mm256_loadu_epi32(x)
            if isinstance(x, List[i32]):
                return Vec._mm256_loadu_epi32(x.arr.ptr)                
        if isinstance(T, i32) and N == 4:
            if isinstance(x, i32):
                return Vec._mm128_set1_epi32(x)
            if isinstance(x, Ptr[i32]):  # TODO: multi-elif does NOT work with statics [why?!]
                return Vec._mm128_loadu_epi32(x)
            if isinstance(x, List[i32]):
                return Vec._mm128_loadu_epi32(x.arr.ptr)                
        compile_error("invalid SIMD vector constructor")

    def __new__(x: str, offset: int = 0) -> Vec[u8, N]:
        return Vec(x.ptr + offset, u8, N)

    def __new__(x: List[T], offset: int = 0) -> Vec[T, N]:
        return Vec(x.arr.ptr + offset, T, N)

    def __new__(x) -> Vec[T, N]:
        return Vec(x, T, N)

    @llvm
    def _mm_cmpeq_epi8(x: Vec[u8, 16], y: Vec[u8, 16]) -> Vec[u8, 16]:
        %0 = icmp eq <16 x i8> %x, %y
        %1 = zext <16 x i1> %0 to <16 x i8>
        ret <16 x i8> %1

    def __eq__(self: Vec[u8, 16], other: Vec[u8, 16]) -> Vec[u8, 16]:
        return Vec._mm_cmpeq_epi8(self, other)

    @llvm
    def _mm512_cmpeq_epi8(x: Vec[u8, 64], y: Vec[u8, 64]) -> Vec[u8, 64]:
        %0 = icmp eq <64 x i8> %x, %y
        %1 = sext <64 x i1> %0 to <64 x i8>
        ret <64 x i8> %1

    def __eq__(self: Vec[u8, 64], other: Vec[u8, 64]) -> Vec[u8, 64]:
        return Vec._mm512_cmpeq_epi8(self, other)

    @llvm
    def _mm256_cmpeq_epi8(x: Vec[u8, 32], y: Vec[u8, 32]) -> Vec[u8, 32]:
        %0 = icmp eq <32 x i8> %x, %y
        %1 = zext <32 x i1> %0 to <32 x i8>
        ret <32 x i8> %1

    def __eq__(self: Vec[u8, 32], other: Vec[u8, 32]) -> Vec[u8, 32]:
        return Vec._mm256_cmpeq_epi8(self, other)

    @llvm
    def _mm_andnot_si128(x: Vec[u8, 16], y: Vec[u8, 16]) -> Vec[u8, 16]:
        %0 = xor <16 x i8> %x, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
        %1 = and <16 x i8> %y, %0
        ret <16 x i8> %1

    def __ne__(self: Vec[u8, 16], other: Vec[u8, 16]) -> Vec[u8, 16]:
        return Vec._mm_andnot_si128((self == other), Vec.FF_16x8i)

    @llvm
    def _mm256_andnot_si256(x: Vec[u8, 32], y: Vec[u8, 32]) -> Vec[u8, 32]:
        %0 = xor <32 x i8> %x, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
        %1 = and <32 x i8> %y, %0
        ret <32 x i8> %1

    def __ne__(self: Vec[u8, 32], other: Vec[u8, 32]) -> Vec[u8, 32]:
        return Vec._mm256_andnot_si256((self == other), Vec.FF_32x8i)

    def __eq__(self: Vec[u8, 16], other: bool) -> Vec[u8, 16]:
        if not other:
            return Vec._mm_andnot_si128(self, Vec.FF_16x8i)
        else:
            return Vec._mm_andnot_si128(self, Vec.ZERO_16x8i)

    def __eq__(self: Vec[u8, 32], other: bool) -> Vec[u8, 32]:
        if not other:
            return Vec._mm256_andnot_si256(self, Vec.FF_32x8i)
        else:
            return Vec._mm256_andnot_si256(self, Vec.ZERO_32x8i)

    @llvm
    def _mm_and_si128(x: Vec[u8, 16], y: Vec[u8, 16]) -> Vec[u8, 16]:
        %0 = and <16 x i8> %x, %y
        ret <16 x i8> %0

    def __and__(self: Vec[u8, 16], other: Vec[u8, 16]) -> Vec[u8, 16]:
        return Vec._mm_and_si128(self, other)

    @llvm
    def _mm_and_si512(x: Vec[u8, 64], y: Vec[u8, 64]) -> Vec[u8, 64]:
        %0 = and <64 x i8> %x, %y
        ret <64 x i8> %0

    def __and__(self: Vec[u8, 64], other: Vec[u8, 64]) -> Vec[u8, 64]:
        return Vec._mm_and_si512(self, other)        

    @llvm
    def _mm_and_si256(x: Vec[u8, 32], y: Vec[u8, 32]) -> Vec[u8, 32]:
        %0 = and <32 x i8> %x, %y
        ret <32 x i8> %0

    def __and__(self: Vec[u8, 32], other: Vec[u8, 32]) -> Vec[u8, 32]:
        return Vec._mm_and_si256(self, other)              

    @llvm
    def _mm256_and_ps(x: Vec[f32, 8], y: Vec[f32, 8]) -> Vec[f32, 8]:
        %0 = bitcast <8 x float> %x to <8 x i32>
        %1 = bitcast <8 x float> %y to <8 x i32>
        %2 = and <8 x i32> %0, %1
        %3 = bitcast <8 x i32> %2 to <8 x float>
        ret <8 x float> %3

    def __and__(self: Vec[f32, 8], other: Vec[f32, 8]) -> Vec[f32, 8]:
        return Vec._mm256_and_ps(self, other)

    @llvm
    def _mm512_and_ps(x: Vec[f32, 16], y: Vec[f32, 16]) -> Vec[f32, 16]:
        %0 = bitcast <16 x float> %x to <16 x i32>
        %1 = bitcast <16 x float> %y to <16 x i32>
        %2 = and <16 x i32> %0, %1
        %3 = bitcast <16 x i32> %2 to <16 x float>
        ret <16 x float> %3

    def __and__(self: Vec[f32, 16], other: Vec[f32, 16]) -> Vec[f32, 16]:
        return Vec._mm512_and_ps(self, other)

    @llvm
    def _mm_or_si128(x: Vec[u8, 16], y: Vec[u8, 16]) -> Vec[u8, 16]:
        %0 = or <16 x i8> %x, %y
        ret <16 x i8> %0

    def __or__(self: Vec[u8, 16], other: Vec[u8, 16]) -> Vec[u8, 16]:
        return Vec._mm_or_si128(self, other)

    @llvm
    def _mm_or_si256(x: Vec[u8, 32], y: Vec[u8, 32]) -> Vec[u8, 32]:
        %0 = or <32 x i8> %x, %y
        ret <32 x i8> %0

    def __or__(self: Vec[u8, 32], other: Vec[u8, 32]) -> Vec[u8, 32]:
        return Vec._mm_or_si256(self, other)

    @llvm
    def _mm256_or_ps(x: Vec[f32, 8], y: Vec[f32, 8]) -> Vec[f32, 8]:
        %0 = bitcast <8 x float> %x to <8 x i32>
        %1 = bitcast <8 x float> %y to <8 x i32>
        %2 = or <8 x i32> %0, %1
        %3 = bitcast <8 x i32> %2 to <8 x float>
        ret <8 x float> %3

    def __or__(self: Vec[f32, 8], other: Vec[f32, 8]) -> Vec[f32, 8]:
        return Vec._mm256_or_ps(self, other)

    @llvm
    def _mm512_or_ps(x: Vec[f32, 16], y: Vec[f32, 16]) -> Vec[f32, 16]:
        %0 = bitcast <16 x float> %x to <16 x i32>
        %1 = bitcast <16 x float> %y to <16 x i32>
        %2 = or <16 x i32> %0, %1
        %3 = bitcast <16 x i32> %2 to <16 x float>
        ret <16 x float> %3

    def __or__(self: Vec[f32, 16], other: Vec[f32, 16]) -> Vec[f32, 16]:
        return Vec._mm512_or_ps(self, other)

    @llvm
    def _mm_bsrli_si128_8(vec: Vec[u8, 16]) -> Vec[u8, 16]:
        %0 = shufflevector <16 x i8> %vec, <16 x i8> zeroinitializer, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
        ret <16 x i8> %0

    @llvm
    def _mm256_add_ps(x: Vec[f32, 8], y: Vec[f32, 8]) -> Vec[f32, 8]:
        %0 = fadd <8 x float> %x, %y
        ret <8 x float> %0

    def __add__(self: Vec[f32, 8], other: Vec[f32, 8]) -> Vec[f32, 8]:
        return Vec._mm256_add_ps(self, other)

    @llvm
    def _mm512_add_epi32(x: Vec[i32, 16], y: Vec[i32, 16]) -> Vec[i32, 16]:
        %0 = add <16 x i32> %x, %y
        ret <16 x i32> %0

    def __add__(self: Vec[i32, 16], other: Vec[i32, 16]) -> Vec[i32, 16]:
        return Vec._mm512_add_epi32(self, other)   

    @llvm
    def _mm256_add_epi16(x: Vec[i16, 16], y: Vec[i16, 16]) -> Vec[i16, 16]:
        %0 = add <16 x i16> %x, %y
        ret <16 x i16> %0

    def __add__(self: Vec[i16, 16], other: Vec[i16, 16]) -> Vec[i16, 16]:
        return Vec._mm256_add_epi16(self, other)   

    @llvm
    def _mm128_add_epi16(x: Vec[i16, 8], y: Vec[i16, 8]) -> Vec[i16, 8]:
        %0 = add <8 x i16> %x, %y
        ret <8 x i16> %0

    def __add__(self: Vec[i16, 8], other: Vec[i16, 8]) -> Vec[i16, 8]:
        return Vec._mm128_add_epi16(self, other)                   

    @llvm
    def _mm1024_add_epi16(x: Vec[i16, 64], y: Vec[i16, 64]) -> Vec[i16, 64]:
        %0 = add <64 x i16> %x, %y
        ret <64 x i16> %0

    def __add__(self: Vec[i16, 64], other: Vec[i16, 64]) -> Vec[i16, 64]:
        return Vec._mm1024_add_epi16(self, other)  

    @llvm
    def _mm512_add_epi16(x: Vec[i16, 32], y: Vec[i16, 32]) -> Vec[i16, 32]:
        %0 = add <32 x i16> %x, %y
        ret <32 x i16> %0

    def __add__(self: Vec[i16, 32], other: Vec[i16, 32]) -> Vec[i16, 32]:
        return Vec._mm512_add_epi16(self, other)

    def __add__(self: Vec[i16, 32], other: List[Vec[i16, 32]]) -> List[Vec[i16, 32]]:        
        for i in range(len(other)):
            other[i] = Vec._mm512_add_epi16(self, other[i])
        return other

    @llvm
    def _mm512_add1_epi32(x: Vec[i32, 16], y: i32) -> Vec[i32, 16]:
        %0 = insertelement <16 x i32> undef, i32 %y, i32 0
        %1 = shufflevector <16 x i32> %0, <16 x i32> undef, <16 x i32> zeroinitializer
        %2 = add <16 x i32> %x, %1
        ret <16 x i32> %2

    def __add__(self: Vec[i32, 16], other: i32) -> Vec[i32, 16]:
        return Vec._mm512_add1_epi32(self, other)  

    @llvm
    def _mm256_add1_epi16(x: Vec[i16, 16], y: i16) -> Vec[i16, 16]:
        %0 = insertelement <16 x i16> undef, i16 %y, i16 0
        %1 = shufflevector <16 x i16> %0, <16 x i16> undef, <16 x i32> zeroinitializer
        %2 = add <16 x i16> %x, %1
        ret <16 x i16> %2

    def __add__(self: Vec[i16, 16], other: i16) -> Vec[i16, 16]:
        return Vec._mm256_add1_epi16(self, other)          

    @llvm
    def _mm512_add1_epi16(x: Vec[i16, 32], y: i16) -> Vec[i16, 32]:
        %0 = insertelement <32 x i16> undef, i16 %y, i16 0
        %1 = shufflevector <32 x i16> %0, <32 x i16> undef, <32 x i32> zeroinitializer
        %2 = add <32 x i16> %x, %1
        ret <32 x i16> %2

    def __add__(self: Vec[i16, 32], other: i16) -> Vec[i16, 32]:
        return Vec._mm512_add1_epi16(self, other)          

    @llvm
    def _mm128_add_epi32(x: Vec[i32, 4], y: Vec[i32, 4]) -> Vec[i32, 4]:
        %0 = add <4 x i32> %x, %y
        ret <4 x i32> %0

    def __add__(self: Vec[i32, 4], other: Vec[i32, 4]) -> Vec[i32, 4]:
        return Vec._mm128_add_epi32(self, other)                

    @llvm
    def _mm512_sub_epi32(x: Vec[i32, 16], y: Vec[i32, 16]) -> Vec[i32, 16]:
        %0 = sub <16 x i32> %x, %y
        ret <16 x i32> %0

    def __sub__(self: Vec[i32, 16], other: Vec[i32, 16]) -> Vec[i32, 16]:
        return Vec._mm512_sub_epi32(self, other)   

    @llvm
    def _mm256_sub_epi16(x: Vec[i16, 16], y: Vec[i16, 16]) -> Vec[i16, 16]:
        %0 = sub <16 x i16> %x, %y
        ret <16 x i16> %0

    def __sub__(self: Vec[i16, 16], other: Vec[i16, 16]) -> Vec[i16, 16]:
        return Vec._mm256_sub_epi16(self, other)   

    @llvm
    def _mm512_sub_epi16(x: Vec[i16, 32], y: Vec[i16, 32]) -> Vec[i16, 32]:
        %0 = sub <32 x i16> %x, %y
        ret <32 x i16> %0

    def __sub__(self: Vec[i16, 32], other: Vec[i16, 32]) -> Vec[i16, 32]:
        return Vec._mm512_sub_epi16(self, other)           

    @llvm
    def _mm1024_sub_epi16(x: Vec[i16, 64], y: Vec[i16, 64]) -> Vec[i16, 64]:
        %0 = sub <64 x i16> %x, %y
        ret <64 x i16> %0

    def __sub__(self: Vec[i16, 64], other: Vec[i16, 64]) -> Vec[i16, 64]:
        return Vec._mm1024_sub_epi16(self, other) 

    @llvm
    def _mm512_sub_epi16(x: Vec[i16, 32], y: Vec[i16, 32]) -> Vec[i16, 32]:
        %0 = sub <32 x i16> %x, %y
        ret <32 x i16> %0

    def __sub__(self: Vec[i16, 32], other: Vec[i16, 32]) -> Vec[i16, 32]:
        return Vec._mm512_sub_epi16(self, other)                   

    @llvm
    def _mm128_sub_epi32(x: Vec[i32, 4], y: Vec[i32, 4]) -> Vec[i32, 4]:
        %0 = sub <4 x i32> %x, %y
        ret <4 x i32> %0

    def __sub__(self: Vec[i32, 4], other: Vec[i32, 4]) -> Vec[i32, 4]:
        return Vec._mm128_sub_epi32(self, other)         

    @llvm
    def _mm256_sub_ps(x: Vec[f32, 8], y: Vec[f32, 8]) -> Vec[f32, 8]:
        %0 = fsub <8 x float> %x, %y
        ret <8 x float> %0

    def __sub__(self: Vec[f32, 8], other: Vec[f32, 8]) -> Vec[f32, 8]:
        return Vec._mm256_sub_ps(self, other)

    def __rshift__(self: Vec[u8, 16], shift: Static[int]) -> Vec[u8, 16]:
        if shift == 0:
            return self
        elif shift == 8:
            return Vec._mm_bsrli_si128_8(self)
        else:
            compile_error("invalid bitshift")

    @llvm
    def _mm512_cmpgt_epi32_mask(x: Vec[i32, 16], y: Vec[i32, 16]) -> Vec[i32, 16]:
        %0 = icmp sgt <16 x i32> %x, %y
        %1 = sext <16 x i1> %0 to <16 x i32>
        ret <16 x i32> %1

    def comparegt(self: Vec[i32, 16], other: Vec[i32, 16]) -> Vec[i32, 16]:
        return Vec._mm512_cmpgt_epi32_mask(self, other)    


    @llvm
    def _mm1024_cmpgt_epi16(x: Vec[i16, 64], y: Vec[i16, 64]) -> Vec[i16, 64]:
        %0 = icmp sgt <64 x i16> %x, %y
        %1 = sext <64 x i1> %0 to <64 x i16>
        ret <64 x i16> %1

    def comparegt(self: Vec[i16, 64], other: Vec[i16, 64]) -> Vec[i16, 64]:
        return Vec._mm1024_cmpgt_epi16(self, other)   
       
    @staticmethod
    @inline
    @llvm
    def _mm512_cmpgt_epi322_am(m3: ptr, m: ptr, index: int, v1: ptr, v2: ptr, h1: ptr, h2: ptr, o: Vec[i16, 32], e: Vec[i16, 32], v: ptr, h: ptr, x: Vec[u8, 32], y, a: Vec[i16, 32], b: Vec[i16, 32], index2: int, am: Vec[i16, 32], n: Vec[u8, 32], ms: ptr) -> None:        
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        declare <32 x i16> @llvm.smax.v32i16(<32 x i16>, <32 x i16>)
        %0 = getelementptr <32 x i16>, ptr %m3, i64 %index
        %1 = getelementptr <32 x i16>, ptr %v1, i64 %index
        %2 = getelementptr <32 x i16>, ptr %v2, i64 %index
        %3 = getelementptr <32 x i16>, ptr %h1, i64 %index
        %4 = getelementptr <32 x i16>, ptr %h2, i64 %index
        %5 = getelementptr <32 x i8>, ptr %y, i64 %index
        %6 = getelementptr <32 x i16>, ptr %m, i64 %index
        %7 = getelementptr <32 x i16>, ptr %v, i64 %index
        %8 = getelementptr <32 x i16>, ptr %h, i64 %index  
        call void @llvm.prefetch(ptr %6, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %7, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %8, i32 1, i32 3, i32 1)         
        %9 = load <32 x i16>, ptr %0
        %10 = load <32 x i16>, ptr %1
        %11 = load <32 x i16>, ptr %2
        %12 = load <32 x i16>, ptr %3
        %13 = load <32 x i16>, ptr %4
        %14 = load <32 x i8>, ptr %5
        %15 = icmp eq <32 x i8> %n, %x
        %16 = zext <32 x i1> %15 to <32 x i16>
        %17 = icmp eq <32 x i8> %n, %14
        %18 = zext <32 x i1> %17 to <32 x i16>
        %19 = or <32 x i16> %16, %18
        %20 = xor <32 x i16> %19, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %21 = mul <32 x i16> %19, %am
        %22 = icmp eq <32 x i8> %x, %14
        %23 = zext <32 x i1> %22 to <32 x i16>
        %24 = mul <32 x i16> %23, %a
        %25 = xor <32 x i16> %23, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %26 = mul <32 x i16> %25, %b
        %27 = add <32 x i16> %24, %26
        %28 = mul <32 x i16> %27, %20
        %29 = add <32 x i16> %28, %21
        %30 = add <32 x i16> %29, %9
        %31 = add <32 x i16> %11, %o
        %32 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %31, <32 x i16> %10)
        %33 = add <32 x i16> %32, %e
        %34 = add <32 x i16> %13, %o
        %35 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %34, <32 x i16> %12)
        %36 = add <32 x i16> %35, %e
        %37 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %33, <32 x i16> %36)
        %38 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %37, <32 x i16> %30)
        store <32 x i16> %38, ptr %6
        store <32 x i16> %33, ptr %7
        store <32 x i16> %36, ptr %8
        %39 = getelementptr <32 x i16>, ptr %m3, i64 %index2
        %40 = getelementptr <32 x i16>, ptr %v1, i64 %index2
        %41 = getelementptr <32 x i16>, ptr %v2, i64 %index2
        %42 = getelementptr <32 x i16>, ptr %h1, i64 %index2
        %43 = getelementptr <32 x i16>, ptr %h2, i64 %index2
        %44 = getelementptr <32 x i8>, ptr %y, i64 %index2
        %45 = getelementptr <32 x i16>, ptr %m, i64 %index2
        %46 = getelementptr <32 x i16>, ptr %v, i64 %index2
        %47 = getelementptr <32 x i16>, ptr %h, i64 %index2 
        call void @llvm.prefetch(ptr %45, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %46, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %47, i32 1, i32 3, i32 1)           
        %48 = load <32 x i16>, ptr %39
        %49 = load <32 x i16>, ptr %40
        %50 = load <32 x i16>, ptr %41
        %51 = load <32 x i16>, ptr %42
        %52 = load <32 x i16>, ptr %43
        %53 = load <32 x i8>, ptr %44
        %54 = icmp eq <32 x i8> %n, %x
        %55 = zext <32 x i1> %54 to <32 x i16>
        %56 = icmp eq <32 x i8> %n, %53
        %57 = zext <32 x i1> %56 to <32 x i16>
        %58 = or <32 x i16> %55, %57
        %59 = xor <32 x i16> %58, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %60 = mul <32 x i16> %58, %am
        %61 = icmp eq <32 x i8> %x, %53
        %62 = zext <32 x i1> %61 to <32 x i16>
        %63 = mul <32 x i16> %62, %a
        %64 = xor <32 x i16> %62, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %65 = mul <32 x i16> %64, %b
        %66 = add <32 x i16> %63, %65
        %67 = mul <32 x i16> %66, %59
        %68 = add <32 x i16> %67, %60
        %69 = add <32 x i16> %68, %48
        %70 = add <32 x i16> %50, %o
        %71 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %70, <32 x i16> %49)
        %72 = add <32 x i16> %71, %e
        %73 = add <32 x i16> %52, %o
        %74 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %73, <32 x i16> %51)
        %75 = add <32 x i16> %74, %e
        %76 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %72, <32 x i16> %75)
        %77 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %76, <32 x i16> %69)
        store <32 x i16> %77, ptr %45
        store <32 x i16> %72, ptr %46
        store <32 x i16> %75, ptr %47
        %78 = getelementptr <32 x i16>, ptr %ms, i64 %index
        %79 = getelementptr <32 x i16>, ptr %ms, i64 %index2
        %80 = load <32 x i16>, ptr %78
        %81 = load <32 x i16>, ptr %79
        %82 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %38, <32 x i16> %80)
        %83 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %77, <32 x i16> %81)
        store <32 x i16> %82, ptr %78
        store <32 x i16> %83, ptr %79
        ret {} {}
    

    @staticmethod    
    @inline
    def comparegt5_am(other2: List[Vec[i16, 32]], other3: List[Vec[i16, 32]], other4: int, other5: List[Vec[i16, 32]], other6: List[Vec[i16, 32]], other7: List[Vec[i16, 32]], other8: List[Vec[i16, 32]], other9: Vec[i16, 32], other10: Vec[i16, 32], other11: List[Vec[i16, 32]], other12: List[Vec[i16, 32]], other13: Vec[u8, 32], other14: List[Vec[u8, 32]], other15: Vec[i16, 32], other16: Vec[i16, 32], other17: Vec[i16, 32], other18: Vec[u8, 32], other19: List[Vec[i16, 32]]):
        Vec._mm512_cmpgt_epi322_am(other2.arr.ptr, other3.arr.ptr, other4, other5.arr.ptr, other6.arr.ptr, other7.arr.ptr, other8.arr.ptr, other9, other10, other11.arr.ptr, other12.arr.ptr, other13, other14.arr.ptr, other15, other16, other4 + 1, other17, other18, other19.arr.ptr)

    @pure
    @derives
    @staticmethod
    @inline
    @llvm
    def _mm512_cmpgt_epi32final_am(m3: ptr, m: ptr, index: int, v1: ptr, v2: ptr, h1: ptr, h2: ptr, o: Vec[i16, 32], e: Vec[i16, 32], v: ptr, h: ptr, x: Vec[u8, 32], y, a: Vec[i16, 32], b: Vec[i16, 32], am: Vec[i16, 32], n: Vec[u8, 32], ms: ptr) -> None:        
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        declare <32 x i16> @llvm.smax.v32i16(<32 x i16>, <32 x i16>)
        %0 = getelementptr <32 x i16>, ptr %m3, i64 %index
        %1 = getelementptr <32 x i16>, ptr %v1, i64 %index
        %2 = getelementptr <32 x i16>, ptr %v2, i64 %index
        %3 = getelementptr <32 x i16>, ptr %h1, i64 %index
        %4 = getelementptr <32 x i16>, ptr %h2, i64 %index
        %5 = getelementptr <32 x i8>, ptr %y, i64 %index
        %6 = getelementptr <32 x i16>, ptr %m, i64 %index
        %7 = getelementptr <32 x i16>, ptr %v, i64 %index
        %8 = getelementptr <32 x i16>, ptr %h, i64 %index  
        call void @llvm.prefetch(ptr %6, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %7, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %8, i32 1, i32 3, i32 1)         
        %9 = load <32 x i16>, ptr %0
        %10 = load <32 x i16>, ptr %1
        %11 = load <32 x i16>, ptr %2
        %12 = load <32 x i16>, ptr %3
        %13 = load <32 x i16>, ptr %4
        %14 = load <32 x i8>, ptr %5
        %15 = icmp eq <32 x i8> %n, %x
        %16 = zext <32 x i1> %15 to <32 x i16>
        %17 = icmp eq <32 x i8> %n, %14
        %18 = zext <32 x i1> %17 to <32 x i16>
        %19 = or <32 x i16> %16, %18
        %20 = xor <32 x i16> %19, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %21 = mul <32 x i16> %19, %am
        %22 = icmp eq <32 x i8> %x, %14
        %23 = zext <32 x i1> %22 to <32 x i16>
        %24 = mul <32 x i16> %23, %a
        %25 = xor <32 x i16> %23, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %26 = mul <32 x i16> %25, %b
        %27 = add <32 x i16> %24, %26
        %28 = mul <32 x i16> %27, %20
        %29 = add <32 x i16> %28, %21
        %30 = add <32 x i16> %29, %9
        %31 = add <32 x i16> %11, %o
        %32 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %31, <32 x i16> %10)
        %33 = add <32 x i16> %32, %e
        %34 = add <32 x i16> %13, %o
        %35 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %34, <32 x i16> %12)
        %36 = add <32 x i16> %35, %e
        %37 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %33, <32 x i16> %36)
        %38 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %37, <32 x i16> %30)
        store <32 x i16> %38, ptr %6
        store <32 x i16> %33, ptr %7
        store <32 x i16> %36, ptr %8
        %39 = getelementptr <32 x i16>, ptr %ms, i64 %index
        %40 = load <32 x i16>, ptr %39
        %41 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %38, <32 x i16> %40)
        store <32 x i16> %41, ptr %39
        ret {} {}

    @staticmethod    
    @inline
    def comparegtfinal_am(other2: List[Vec[i16, 32]], other3: List[Vec[i16, 32]], other4: int, other5: List[Vec[i16, 32]], other6: List[Vec[i16, 32]], other7: List[Vec[i16, 32]], other8: List[Vec[i16, 32]], other9: Vec[i16, 32], other10: Vec[i16, 32], other11: List[Vec[i16, 32]], other12: List[Vec[i16, 32]], other13: Vec[u8, 32], other14: List[Vec[u8, 32]], other15: Vec[i16, 32], other16: Vec[i16, 32], other17: Vec[i16, 32], other18: Vec[u8, 32], other19: List[Vec[i16, 32]]):
        Vec._mm512_cmpgt_epi32final_am(other2.arr.ptr, other3.arr.ptr, other4, other5.arr.ptr, other6.arr.ptr, other7.arr.ptr, other8.arr.ptr, other9, other10, other11.arr.ptr, other12.arr.ptr, other13, other14.arr.ptr, other15, other16, other17, other18, other19.arr.ptr)        

    @staticmethod
    @inline
    @llvm
    def _mm512_cmpgt_epi322(m3: ptr, m: ptr, index: int, v1: ptr, v2: ptr, h1: ptr, h2: ptr, o: Vec[i16, 32], e: Vec[i16, 32], v: ptr, h: ptr, x: Vec[u8, 32], y, a: Vec[i16, 32], b: Vec[i16, 32], index2: int) -> None:        
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        declare <32 x i16> @llvm.smax.v32i16(<32 x i16>, <32 x i16>)
        %0 = getelementptr <32 x i16>, ptr %m3, i64 %index
        %1 = getelementptr <32 x i16>, ptr %v1, i64 %index
        %2 = getelementptr <32 x i16>, ptr %v2, i64 %index
        %3 = getelementptr <32 x i16>, ptr %h1, i64 %index
        %4 = getelementptr <32 x i16>, ptr %h2, i64 %index
        %5 = getelementptr <32 x i8>, ptr %y, i64 %index
        %6 = getelementptr <32 x i16>, ptr %m, i64 %index
        %7 = getelementptr <32 x i16>, ptr %v, i64 %index
        %8 = getelementptr <32 x i16>, ptr %h, i64 %index  
        call void @llvm.prefetch(ptr %6, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %7, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %8, i32 1, i32 3, i32 1)         
        %9 = load <32 x i16>, ptr %0
        %10 = load <32 x i16>, ptr %1
        %11 = load <32 x i16>, ptr %2
        %12 = load <32 x i16>, ptr %3
        %13 = load <32 x i16>, ptr %4
        %14 = load <32 x i8>, ptr %5
        %15 = icmp eq <32 x i8> %x, %14
        %16 = zext <32 x i1> %15 to <32 x i16>
        %17 = mul <32 x i16> %16, %a
        %18 = xor <32 x i16> %16, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %19 = mul <32 x i16> %18, %b
        %20 = add <32 x i16> %17, %19
        %21 = add <32 x i16> %20, %9
        %22 = add <32 x i16> %11, %o
        %23 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %22, <32 x i16> %10)
        %24 = add <32 x i16> %23, %e
        %25 = add <32 x i16> %13, %o
        %26 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %25, <32 x i16> %12)
        %27 = add <32 x i16> %26, %e
        %28 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %24, <32 x i16> %27)
        %29 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %28, <32 x i16> %21)
        store <32 x i16> %29, ptr %6
        store <32 x i16> %24, ptr %7
        store <32 x i16> %27, ptr %8
        %30 = getelementptr <32 x i16>, ptr %m3, i64 %index2
        %31 = getelementptr <32 x i16>, ptr %v1, i64 %index2
        %32 = getelementptr <32 x i16>, ptr %v2, i64 %index2
        %33 = getelementptr <32 x i16>, ptr %h1, i64 %index2
        %34 = getelementptr <32 x i16>, ptr %h2, i64 %index2
        %35 = getelementptr <32 x i8>, ptr %y, i64 %index2
        %36 = getelementptr <32 x i16>, ptr %m, i64 %index2
        %37 = getelementptr <32 x i16>, ptr %v, i64 %index2
        %38 = getelementptr <32 x i16>, ptr %h, i64 %index2 
        call void @llvm.prefetch(ptr %36, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %37, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %38, i32 1, i32 3, i32 1)         
        %39 = load <32 x i16>, ptr %30
        %40 = load <32 x i16>, ptr %31
        %41 = load <32 x i16>, ptr %32
        %42 = load <32 x i16>, ptr %33
        %43 = load <32 x i16>, ptr %34
        %44 = load <32 x i8>, ptr %35
        %45 = icmp eq <32 x i8> %x, %44
        %46 = zext <32 x i1> %45 to <32 x i16>
        %47 = mul <32 x i16> %46, %a
        %48 = xor <32 x i16> %46, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %49 = mul <32 x i16> %48, %b
        %50 = add <32 x i16> %47, %49
        %51 = add <32 x i16> %50, %39
        %52 = add <32 x i16> %41, %o
        %53 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %52, <32 x i16> %40)
        %54 = add <32 x i16> %53, %e
        %55 = add <32 x i16> %43, %o
        %56 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %55, <32 x i16> %42)
        %57 = add <32 x i16> %56, %e
        %58 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %54, <32 x i16> %57)
        %59 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %58, <32 x i16> %51)
        store <32 x i16> %59, ptr %36
        store <32 x i16> %54, ptr %37
        store <32 x i16> %57, ptr %38        
        ret {} {}
    

    @staticmethod    
    @inline
    def comparegt5(other2: List[Vec[i16, 32]], other3: List[Vec[i16, 32]], other4: int, other5: List[Vec[i16, 32]], other6: List[Vec[i16, 32]], other7: List[Vec[i16, 32]], other8: List[Vec[i16, 32]], other9: Vec[i16, 32], other10: Vec[i16, 32], other11: List[Vec[i16, 32]], other12: List[Vec[i16, 32]], other13: Vec[u8, 32], other14: List[Vec[u8, 32]], other15: Vec[i16, 32], other16: Vec[i16, 32]):
        Vec._mm512_cmpgt_epi322(other2.arr.ptr, other3.arr.ptr, other4, other5.arr.ptr, other6.arr.ptr, other7.arr.ptr, other8.arr.ptr, other9, other10, other11.arr.ptr, other12.arr.ptr, other13, other14.arr.ptr, other15, other16, other4 + 1)

    @pure
    @derives
    @staticmethod
    @inline
    @llvm
    def _mm512_cmpgt_epi32final(m3: ptr, m: ptr, index: int, v1: ptr, v2: ptr, h1: ptr, h2: ptr, o: Vec[i16, 32], e: Vec[i16, 32], v: ptr, h: ptr, x: Vec[u8, 32], y, a: Vec[i16, 32], b: Vec[i16, 32]) -> None:        
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        declare <32 x i16> @llvm.smax.v32i16(<32 x i16>, <32 x i16>)
        %0 = getelementptr <32 x i16>, ptr %m3, i64 %index
        %1 = getelementptr <32 x i16>, ptr %v1, i64 %index
        %2 = getelementptr <32 x i16>, ptr %v2, i64 %index
        %3 = getelementptr <32 x i16>, ptr %h1, i64 %index
        %4 = getelementptr <32 x i16>, ptr %h2, i64 %index
        %5 = getelementptr <32 x i8>, ptr %y, i64 %index
        %6 = getelementptr <32 x i16>, ptr %m, i64 %index
        %7 = getelementptr <32 x i16>, ptr %v, i64 %index
        %8 = getelementptr <32 x i16>, ptr %h, i64 %index  
        call void @llvm.prefetch(ptr %6, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %7, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %8, i32 1, i32 3, i32 1)         
        %9 = load <32 x i16>, ptr %0
        %10 = load <32 x i16>, ptr %1
        %11 = load <32 x i16>, ptr %2
        %12 = load <32 x i16>, ptr %3
        %13 = load <32 x i16>, ptr %4
        %14 = load <32 x i8>, ptr %5
        %15 = icmp eq <32 x i8> %x, %14
        %16 = zext <32 x i1> %15 to <32 x i16>
        %17 = mul <32 x i16> %16, %a
        %18 = xor <32 x i16> %16, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %19 = mul <32 x i16> %18, %b
        %20 = add <32 x i16> %17, %19
        %21 = add <32 x i16> %20, %9
        %22 = add <32 x i16> %11, %o
        %23 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %22, <32 x i16> %10)
        %24 = add <32 x i16> %23, %e
        %25 = add <32 x i16> %13, %o
        %26 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %25, <32 x i16> %12)
        %27 = add <32 x i16> %26, %e
        %28 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %24, <32 x i16> %27)
        %29 = call <32 x i16> @llvm.smax.v32i16(<32 x i16> %28, <32 x i16> %21)
        store <32 x i16> %29, ptr %6
        store <32 x i16> %24, ptr %7
        store <32 x i16> %27, ptr %8
        ret {} {}

    @staticmethod    
    @inline
    def comparegtfinal(other2: List[Vec[i16, 32]], other3: List[Vec[i16, 32]], other4: int, other5: List[Vec[i16, 32]], other6: List[Vec[i16, 32]], other7: List[Vec[i16, 32]], other8: List[Vec[i16, 32]], other9: Vec[i16, 32], other10: Vec[i16, 32], other11: List[Vec[i16, 32]], other12: List[Vec[i16, 32]], other13: Vec[u8, 32], other14: List[Vec[u8, 32]], other15: Vec[i16, 32], other16: Vec[i16, 32]):
        Vec._mm512_cmpgt_epi32final(other2.arr.ptr, other3.arr.ptr, other4, other5.arr.ptr, other6.arr.ptr, other7.arr.ptr, other8.arr.ptr, other9, other10, other11.arr.ptr, other12.arr.ptr, other13, other14.arr.ptr, other15, other16)        

    @staticmethod
    @inline
    @llvm
    def _mm256_cmpgt_epi162_am(m3: ptr, m: ptr, index: int, v1: ptr, v2: ptr, h1: ptr, h2: ptr, o: Vec[i16, 16], e: Vec[i16, 16], v: ptr, h: ptr, x: Vec[u8, 16], y, a: Vec[i16, 16], b: Vec[i16, 16], index2: int, am: Vec[i16, 16], n: Vec[u8, 16], ms: ptr) -> None:        
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        declare <16 x i16> @llvm.smax.v16i16(<16 x i16>, <16 x i16>)
        %0 = getelementptr <16 x i16>, ptr %m3, i64 %index
        %1 = getelementptr <16 x i16>, ptr %v1, i64 %index
        %2 = getelementptr <16 x i16>, ptr %v2, i64 %index
        %3 = getelementptr <16 x i16>, ptr %h1, i64 %index
        %4 = getelementptr <16 x i16>, ptr %h2, i64 %index
        %5 = getelementptr <16 x i8>, ptr %y, i64 %index
        %6 = getelementptr <16 x i16>, ptr %m, i64 %index
        %7 = getelementptr <16 x i16>, ptr %v, i64 %index
        %8 = getelementptr <16 x i16>, ptr %h, i64 %index  
        call void @llvm.prefetch(ptr %6, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %7, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %8, i32 1, i32 3, i32 1)         
        %9 = load <16 x i16>, ptr %0
        %10 = load <16 x i16>, ptr %1
        %11 = load <16 x i16>, ptr %2
        %12 = load <16 x i16>, ptr %3
        %13 = load <16 x i16>, ptr %4
        %14 = load <16 x i8>, ptr %5
        %15 = icmp eq <16 x i8> %n, %x
        %16 = zext <16 x i1> %15 to <16 x i16>
        %17 = icmp eq <16 x i8> %n, %14
        %18 = zext <16 x i1> %17 to <16 x i16>
        %19 = or <16 x i16> %16, %18
        %20 = xor <16 x i16> %19, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %21 = mul <16 x i16> %19, %am
        %22 = icmp eq <16 x i8> %x, %14
        %23 = zext <16 x i1> %22 to <16 x i16>
        %24 = mul <16 x i16> %23, %a
        %25 = xor <16 x i16> %23, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %26 = mul <16 x i16> %25, %b
        %27 = add <16 x i16> %24, %26
        %28 = mul <16 x i16> %27, %20
        %29 = add <16 x i16> %28, %21
        %30 = add <16 x i16> %29, %9
        %31 = add <16 x i16> %11, %o
        %32 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %31, <16 x i16> %10)
        %33 = add <16 x i16> %32, %e
        %34 = add <16 x i16> %13, %o
        %35 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %34, <16 x i16> %12)
        %36 = add <16 x i16> %35, %e
        %37 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %33, <16 x i16> %36)
        %38 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %37, <16 x i16> %30)
        store <16 x i16> %38, ptr %6
        store <16 x i16> %33, ptr %7
        store <16 x i16> %36, ptr %8
        %39 = getelementptr <16 x i16>, ptr %m3, i64 %index2
        %40 = getelementptr <16 x i16>, ptr %v1, i64 %index2
        %41 = getelementptr <16 x i16>, ptr %v2, i64 %index2
        %42 = getelementptr <16 x i16>, ptr %h1, i64 %index2
        %43 = getelementptr <16 x i16>, ptr %h2, i64 %index2
        %44 = getelementptr <16 x i8>, ptr %y, i64 %index2
        %45 = getelementptr <16 x i16>, ptr %m, i64 %index2
        %46 = getelementptr <16 x i16>, ptr %v, i64 %index2
        %47 = getelementptr <16 x i16>, ptr %h, i64 %index2 
        call void @llvm.prefetch(ptr %45, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %46, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %47, i32 1, i32 3, i32 1)           
        %48 = load <16 x i16>, ptr %39
        %49 = load <16 x i16>, ptr %40
        %50 = load <16 x i16>, ptr %41
        %51 = load <16 x i16>, ptr %42
        %52 = load <16 x i16>, ptr %43
        %53 = load <16 x i8>, ptr %44
        %54 = icmp eq <16 x i8> %n, %x
        %55 = zext <16 x i1> %54 to <16 x i16>
        %56 = icmp eq <16 x i8> %n, %53
        %57 = zext <16 x i1> %56 to <16 x i16>
        %58 = or <16 x i16> %55, %57
        %59 = xor <16 x i16> %58, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %60 = mul <16 x i16> %58, %am
        %61 = icmp eq <16 x i8> %x, %53
        %62 = zext <16 x i1> %61 to <16 x i16>
        %63 = mul <16 x i16> %62, %a
        %64 = xor <16 x i16> %62, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %65 = mul <16 x i16> %64, %b
        %66 = add <16 x i16> %63, %65
        %67 = mul <16 x i16> %66, %59
        %68 = add <16 x i16> %67, %60
        %69 = add <16 x i16> %68, %48
        %70 = add <16 x i16> %50, %o
        %71 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %70, <16 x i16> %49)
        %72 = add <16 x i16> %71, %e
        %73 = add <16 x i16> %52, %o
        %74 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %73, <16 x i16> %51)
        %75 = add <16 x i16> %74, %e
        %76 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %72, <16 x i16> %75)
        %77 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %76, <16 x i16> %69)
        store <16 x i16> %77, ptr %45
        store <16 x i16> %72, ptr %46
        store <16 x i16> %75, ptr %47
        %78 = getelementptr <16 x i16>, ptr %ms, i64 %index
        %79 = getelementptr <16 x i16>, ptr %ms, i64 %index2
        %80 = load <16 x i16>, ptr %78
        %81 = load <16 x i16>, ptr %79
        %82 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %38, <16 x i16> %80)
        %83 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %77, <16 x i16> %81)
        store <16 x i16> %82, ptr %78
        store <16 x i16> %83, ptr %79
        ret {} {}
    

    @staticmethod    
    @inline
    def comparegt5_am(other2: List[Vec[i16, 16]], other3: List[Vec[i16, 16]], other4: int, other5: List[Vec[i16, 16]], other6: List[Vec[i16, 16]], other7: List[Vec[i16, 16]], other8: List[Vec[i16, 16]], other9: Vec[i16, 16], other10: Vec[i16, 16], other11: List[Vec[i16, 16]], other12: List[Vec[i16, 16]], other13: Vec[u8, 16], other14: List[Vec[u8, 16]], other15: Vec[i16, 16], other16: Vec[i16, 16], other17: Vec[i16, 16], other18: Vec[u8, 16], other19: List[Vec[i16, 16]]):
        Vec._mm256_cmpgt_epi162_am(other2.arr.ptr, other3.arr.ptr, other4, other5.arr.ptr, other6.arr.ptr, other7.arr.ptr, other8.arr.ptr, other9, other10, other11.arr.ptr, other12.arr.ptr, other13, other14.arr.ptr, other15, other16, other4 + 1, other17, other18, other19.arr.ptr)

    @pure
    @derives
    @staticmethod
    @inline
    @llvm
    def _mm256_cmpgt_epi16final_am(m3: ptr, m: ptr, index: int, v1: ptr, v2: ptr, h1: ptr, h2: ptr, o: Vec[i16, 16], e: Vec[i16, 16], v: ptr, h: ptr, x: Vec[u8, 16], y, a: Vec[i16, 16], b: Vec[i16, 16], am: Vec[i16, 16], n: Vec[u8, 16], ms: ptr) -> None:        
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        declare <16 x i16> @llvm.smax.v16i16(<16 x i16>, <16 x i16>)
        %0 = getelementptr <16 x i16>, ptr %m3, i64 %index
        %1 = getelementptr <16 x i16>, ptr %v1, i64 %index
        %2 = getelementptr <16 x i16>, ptr %v2, i64 %index
        %3 = getelementptr <16 x i16>, ptr %h1, i64 %index
        %4 = getelementptr <16 x i16>, ptr %h2, i64 %index
        %5 = getelementptr <16 x i8>, ptr %y, i64 %index
        %6 = getelementptr <16 x i16>, ptr %m, i64 %index
        %7 = getelementptr <16 x i16>, ptr %v, i64 %index
        %8 = getelementptr <16 x i16>, ptr %h, i64 %index  
        call void @llvm.prefetch(ptr %6, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %7, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %8, i32 1, i32 3, i32 1)         
        %9 = load <16 x i16>, ptr %0
        %10 = load <16 x i16>, ptr %1
        %11 = load <16 x i16>, ptr %2
        %12 = load <16 x i16>, ptr %3
        %13 = load <16 x i16>, ptr %4
        %14 = load <16 x i8>, ptr %5
        %15 = icmp eq <16 x i8> %n, %x
        %16 = zext <16 x i1> %15 to <16 x i16>
        %17 = icmp eq <16 x i8> %n, %14
        %18 = zext <16 x i1> %17 to <16 x i16>
        %19 = or <16 x i16> %16, %18
        %20 = xor <16 x i16> %19, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %21 = mul <16 x i16> %19, %am
        %22 = icmp eq <16 x i8> %x, %14
        %23 = zext <16 x i1> %22 to <16 x i16>
        %24 = mul <16 x i16> %23, %a
        %25 = xor <16 x i16> %23, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %26 = mul <16 x i16> %25, %b
        %27 = add <16 x i16> %24, %26
        %28 = mul <16 x i16> %27, %20
        %29 = add <16 x i16> %28, %21
        %30 = add <16 x i16> %29, %9
        %31 = add <16 x i16> %11, %o
        %32 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %31, <16 x i16> %10)
        %33 = add <16 x i16> %32, %e
        %34 = add <16 x i16> %13, %o
        %35 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %34, <16 x i16> %12)
        %36 = add <16 x i16> %35, %e
        %37 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %33, <16 x i16> %36)
        %38 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %37, <16 x i16> %30)
        store <16 x i16> %38, ptr %6
        store <16 x i16> %33, ptr %7
        store <16 x i16> %36, ptr %8
        %39 = getelementptr <16 x i16>, ptr %ms, i64 %index
        %40 = load <16 x i16>, ptr %39
        %41 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %38, <16 x i16> %40)
        store <16 x i16> %41, ptr %39
        ret {} {}

    @staticmethod    
    @inline
    def comparegtfinal_am(other2: List[Vec[i16, 16]], other3: List[Vec[i16, 16]], other4: int, other5: List[Vec[i16, 16]], other6: List[Vec[i16, 16]], other7: List[Vec[i16, 16]], other8: List[Vec[i16, 16]], other9: Vec[i16, 16], other10: Vec[i16, 16], other11: List[Vec[i16, 16]], other12: List[Vec[i16, 16]], other13: Vec[u8, 16], other14: List[Vec[u8, 16]], other15: Vec[i16, 16], other16: Vec[i16, 16], other17: Vec[i16, 16], other18: Vec[u8, 16], other19: List[Vec[i16, 16]]):
        Vec._mm256_cmpgt_epi16final_am(other2.arr.ptr, other3.arr.ptr, other4, other5.arr.ptr, other6.arr.ptr, other7.arr.ptr, other8.arr.ptr, other9, other10, other11.arr.ptr, other12.arr.ptr, other13, other14.arr.ptr, other15, other16, other17, other18, other19.arr.ptr)        

    @staticmethod
    @inline
    @llvm
    def _mm256_cmpgt_epi162(m3: ptr, m: ptr, index: int, v1: ptr, v2: ptr, h1: ptr, h2: ptr, o: Vec[i16, 16], e: Vec[i16, 16], v: ptr, h: ptr, x: Vec[u8, 16], y, a: Vec[i16, 16], b: Vec[i16, 16], index2: int) -> None:        
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        declare <16 x i16> @llvm.smax.v16i16(<16 x i16>, <16 x i16>)
        %0 = getelementptr <16 x i16>, ptr %m3, i64 %index
        %1 = getelementptr <16 x i16>, ptr %v1, i64 %index
        %2 = getelementptr <16 x i16>, ptr %v2, i64 %index
        %3 = getelementptr <16 x i16>, ptr %h1, i64 %index
        %4 = getelementptr <16 x i16>, ptr %h2, i64 %index
        %5 = getelementptr <16 x i8>, ptr %y, i64 %index
        %6 = getelementptr <16 x i16>, ptr %m, i64 %index
        %7 = getelementptr <16 x i16>, ptr %v, i64 %index
        %8 = getelementptr <16 x i16>, ptr %h, i64 %index  
        call void @llvm.prefetch(ptr %6, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %7, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %8, i32 1, i32 3, i32 1)         
        %9 = load <16 x i16>, ptr %0
        %10 = load <16 x i16>, ptr %1
        %11 = load <16 x i16>, ptr %2
        %12 = load <16 x i16>, ptr %3
        %13 = load <16 x i16>, ptr %4
        %14 = load <16 x i8>, ptr %5
        %15 = icmp eq <16 x i8> %x, %14
        %16 = zext <16 x i1> %15 to <16 x i16>
        %17 = mul <16 x i16> %16, %a
        %18 = xor <16 x i16> %16, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %19 = mul <16 x i16> %18, %b
        %20 = add <16 x i16> %17, %19
        %21 = add <16 x i16> %20, %9
        %22 = add <16 x i16> %11, %o
        %23 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %22, <16 x i16> %10)
        %24 = add <16 x i16> %23, %e
        %25 = add <16 x i16> %13, %o
        %26 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %25, <16 x i16> %12)
        %27 = add <16 x i16> %26, %e
        %28 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %24, <16 x i16> %27)
        %29 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %28, <16 x i16> %21)
        store <16 x i16> %29, ptr %6
        store <16 x i16> %24, ptr %7
        store <16 x i16> %27, ptr %8
        %30 = getelementptr <16 x i16>, ptr %m3, i64 %index2
        %31 = getelementptr <16 x i16>, ptr %v1, i64 %index2
        %32 = getelementptr <16 x i16>, ptr %v2, i64 %index2
        %33 = getelementptr <16 x i16>, ptr %h1, i64 %index2
        %34 = getelementptr <16 x i16>, ptr %h2, i64 %index2
        %35 = getelementptr <16 x i8>, ptr %y, i64 %index2
        %36 = getelementptr <16 x i16>, ptr %m, i64 %index2
        %37 = getelementptr <16 x i16>, ptr %v, i64 %index2
        %38 = getelementptr <16 x i16>, ptr %h, i64 %index2 
        call void @llvm.prefetch(ptr %36, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %37, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %38, i32 1, i32 3, i32 1)         
        %39 = load <16 x i16>, ptr %30
        %40 = load <16 x i16>, ptr %31
        %41 = load <16 x i16>, ptr %32
        %42 = load <16 x i16>, ptr %33
        %43 = load <16 x i16>, ptr %34
        %44 = load <16 x i8>, ptr %35
        %45 = icmp eq <16 x i8> %x, %44
        %46 = zext <16 x i1> %45 to <16 x i16>
        %47 = mul <16 x i16> %46, %a
        %48 = xor <16 x i16> %46, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %49 = mul <16 x i16> %48, %b
        %50 = add <16 x i16> %47, %49
        %51 = add <16 x i16> %50, %39
        %52 = add <16 x i16> %41, %o
        %53 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %52, <16 x i16> %40)
        %54 = add <16 x i16> %53, %e
        %55 = add <16 x i16> %43, %o
        %56 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %55, <16 x i16> %42)
        %57 = add <16 x i16> %56, %e
        %58 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %54, <16 x i16> %57)
        %59 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %58, <16 x i16> %51)
        store <16 x i16> %59, ptr %36
        store <16 x i16> %54, ptr %37
        store <16 x i16> %57, ptr %38        
        ret {} {}
    

    @staticmethod    
    @inline
    def comparegt5(other2: List[Vec[i16, 16]], other3: List[Vec[i16, 16]], other4: int, other5: List[Vec[i16, 16]], other6: List[Vec[i16, 16]], other7: List[Vec[i16, 16]], other8: List[Vec[i16, 16]], other9: Vec[i16, 16], other10: Vec[i16, 16], other11: List[Vec[i16, 16]], other12: List[Vec[i16, 16]], other13: Vec[u8, 16], other14: List[Vec[u8, 16]], other15: Vec[i16, 16], other16: Vec[i16, 16]):
        Vec._mm256_cmpgt_epi162(other2.arr.ptr, other3.arr.ptr, other4, other5.arr.ptr, other6.arr.ptr, other7.arr.ptr, other8.arr.ptr, other9, other10, other11.arr.ptr, other12.arr.ptr, other13, other14.arr.ptr, other15, other16, other4 + 1)

    @pure
    @derives
    @staticmethod
    @inline
    @llvm
    def _mm256_cmpgt_epi16final(m3: ptr, m: ptr, index: int, v1: ptr, v2: ptr, h1: ptr, h2: ptr, o: Vec[i16, 16], e: Vec[i16, 16], v: ptr, h: ptr, x: Vec[u8, 16], y, a: Vec[i16, 16], b: Vec[i16, 16]) -> None:        
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        declare <16 x i16> @llvm.smax.v16i16(<16 x i16>, <16 x i16>)
        %0 = getelementptr <16 x i16>, ptr %m3, i64 %index
        %1 = getelementptr <16 x i16>, ptr %v1, i64 %index
        %2 = getelementptr <16 x i16>, ptr %v2, i64 %index
        %3 = getelementptr <16 x i16>, ptr %h1, i64 %index
        %4 = getelementptr <16 x i16>, ptr %h2, i64 %index
        %5 = getelementptr <16 x i8>, ptr %y, i64 %index
        %6 = getelementptr <16 x i16>, ptr %m, i64 %index
        %7 = getelementptr <16 x i16>, ptr %v, i64 %index
        %8 = getelementptr <16 x i16>, ptr %h, i64 %index  
        call void @llvm.prefetch(ptr %6, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %7, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %8, i32 1, i32 3, i32 1)         
        %9 = load <16 x i16>, ptr %0
        %10 = load <16 x i16>, ptr %1
        %11 = load <16 x i16>, ptr %2
        %12 = load <16 x i16>, ptr %3
        %13 = load <16 x i16>, ptr %4
        %14 = load <16 x i8>, ptr %5
        %15 = icmp eq <16 x i8> %x, %14
        %16 = zext <16 x i1> %15 to <16 x i16>
        %17 = mul <16 x i16> %16, %a
        %18 = xor <16 x i16> %16, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %19 = mul <16 x i16> %18, %b
        %20 = add <16 x i16> %17, %19
        %21 = add <16 x i16> %20, %9
        %22 = add <16 x i16> %11, %o
        %23 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %22, <16 x i16> %10)
        %24 = add <16 x i16> %23, %e
        %25 = add <16 x i16> %13, %o
        %26 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %25, <16 x i16> %12)
        %27 = add <16 x i16> %26, %e
        %28 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %24, <16 x i16> %27)
        %29 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %28, <16 x i16> %21)
        store <16 x i16> %29, ptr %6
        store <16 x i16> %24, ptr %7
        store <16 x i16> %27, ptr %8
        ret {} {}

    @staticmethod    
    @inline
    def comparegtfinal(other2: List[Vec[i16, 16]], other3: List[Vec[i16, 16]], other4: int, other5: List[Vec[i16, 16]], other6: List[Vec[i16, 16]], other7: List[Vec[i16, 16]], other8: List[Vec[i16, 16]], other9: Vec[i16, 16], other10: Vec[i16, 16], other11: List[Vec[i16, 16]], other12: List[Vec[i16, 16]], other13: Vec[u8, 16], other14: List[Vec[u8, 16]], other15: Vec[i16, 16], other16: Vec[i16, 16]):
        Vec._mm256_cmpgt_epi16final(other2.arr.ptr, other3.arr.ptr, other4, other5.arr.ptr, other6.arr.ptr, other7.arr.ptr, other8.arr.ptr, other9, other10, other11.arr.ptr, other12.arr.ptr, other13, other14.arr.ptr, other15, other16)        


    @staticmethod
    @inline
    @llvm
    def _mm128_cmpgt_epi82_am(m3: ptr, m: ptr, index: int, v1: ptr, v2: ptr, h1: ptr, h2: ptr, o: Vec[i16, 8], e: Vec[i16, 8], v: ptr, h: ptr, x: Vec[u8, 8], y, a: Vec[i16, 8], b: Vec[i16, 8], index2: int, am: Vec[i16, 8], n: Vec[u8, 8], ms: ptr) -> None:        
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        declare <8 x i16> @llvm.smax.v8i16(<8 x i16>, <8 x i16>)
        %0 = getelementptr <8 x i16>, ptr %m3, i64 %index
        %1 = getelementptr <8 x i16>, ptr %v1, i64 %index
        %2 = getelementptr <8 x i16>, ptr %v2, i64 %index
        %3 = getelementptr <8 x i16>, ptr %h1, i64 %index
        %4 = getelementptr <8 x i16>, ptr %h2, i64 %index
        %5 = getelementptr <8 x i8>, ptr %y, i64 %index
        %6 = getelementptr <8 x i16>, ptr %m, i64 %index
        %7 = getelementptr <8 x i16>, ptr %v, i64 %index
        %8 = getelementptr <8 x i16>, ptr %h, i64 %index  
        call void @llvm.prefetch(ptr %6, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %7, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %8, i32 1, i32 3, i32 1)         
        %9 = load <8 x i16>, ptr %0
        %10 = load <8 x i16>, ptr %1
        %11 = load <8 x i16>, ptr %2
        %12 = load <8 x i16>, ptr %3
        %13 = load <8 x i16>, ptr %4
        %14 = load <8 x i8>, ptr %5
        %15 = icmp eq <8 x i8> %n, %x
        %16 = zext <8 x i1> %15 to <8 x i16>
        %17 = icmp eq <8 x i8> %n, %14
        %18 = zext <8 x i1> %17 to <8 x i16>
        %19 = or <8 x i16> %16, %18
        %20 = xor <8 x i16> %19, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %21 = mul <8 x i16> %19, %am
        %22 = icmp eq <8 x i8> %x, %14
        %23 = zext <8 x i1> %22 to <8 x i16>
        %24 = mul <8 x i16> %23, %a
        %25 = xor <8 x i16> %23, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %26 = mul <8 x i16> %25, %b
        %27 = add <8 x i16> %24, %26
        %28 = mul <8 x i16> %27, %20
        %29 = add <8 x i16> %28, %21
        %30 = add <8 x i16> %29, %9
        %31 = add <8 x i16> %11, %o
        %32 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %31, <8 x i16> %10)
        %33 = add <8 x i16> %32, %e
        %34 = add <8 x i16> %13, %o
        %35 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %34, <8 x i16> %12)
        %36 = add <8 x i16> %35, %e
        %37 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %33, <8 x i16> %36)
        %38 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %37, <8 x i16> %30)
        store <8 x i16> %38, ptr %6
        store <8 x i16> %33, ptr %7
        store <8 x i16> %36, ptr %8
        %39 = getelementptr <8 x i16>, ptr %m3, i64 %index2
        %40 = getelementptr <8 x i16>, ptr %v1, i64 %index2
        %41 = getelementptr <8 x i16>, ptr %v2, i64 %index2
        %42 = getelementptr <8 x i16>, ptr %h1, i64 %index2
        %43 = getelementptr <8 x i16>, ptr %h2, i64 %index2
        %44 = getelementptr <8 x i8>, ptr %y, i64 %index2
        %45 = getelementptr <8 x i16>, ptr %m, i64 %index2
        %46 = getelementptr <8 x i16>, ptr %v, i64 %index2
        %47 = getelementptr <8 x i16>, ptr %h, i64 %index2 
        call void @llvm.prefetch(ptr %45, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %46, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %47, i32 1, i32 3, i32 1)           
        %48 = load <8 x i16>, ptr %39
        %49 = load <8 x i16>, ptr %40
        %50 = load <8 x i16>, ptr %41
        %51 = load <8 x i16>, ptr %42
        %52 = load <8 x i16>, ptr %43
        %53 = load <8 x i8>, ptr %44
        %54 = icmp eq <8 x i8> %n, %x
        %55 = zext <8 x i1> %54 to <8 x i16>
        %56 = icmp eq <8 x i8> %n, %53
        %57 = zext <8 x i1> %56 to <8 x i16>
        %58 = or <8 x i16> %55, %57
        %59 = xor <8 x i16> %58, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %60 = mul <8 x i16> %58, %am
        %61 = icmp eq <8 x i8> %x, %53
        %62 = zext <8 x i1> %61 to <8 x i16>
        %63 = mul <8 x i16> %62, %a
        %64 = xor <8 x i16> %62, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %65 = mul <8 x i16> %64, %b
        %66 = add <8 x i16> %63, %65
        %67 = mul <8 x i16> %66, %59
        %68 = add <8 x i16> %67, %60
        %69 = add <8 x i16> %68, %48
        %70 = add <8 x i16> %50, %o
        %71 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %70, <8 x i16> %49)
        %72 = add <8 x i16> %71, %e
        %73 = add <8 x i16> %52, %o
        %74 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %73, <8 x i16> %51)
        %75 = add <8 x i16> %74, %e
        %76 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %72, <8 x i16> %75)
        %77 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %76, <8 x i16> %69)
        store <8 x i16> %77, ptr %45
        store <8 x i16> %72, ptr %46
        store <8 x i16> %75, ptr %47
        %78 = getelementptr <8 x i16>, ptr %ms, i64 %index
        %79 = getelementptr <8 x i16>, ptr %ms, i64 %index2
        %80 = load <8 x i16>, ptr %78
        %81 = load <8 x i16>, ptr %79
        %82 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %38, <8 x i16> %80)
        %83 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %77, <8 x i16> %81)
        store <8 x i16> %82, ptr %78
        store <8 x i16> %83, ptr %79
        ret {} {}
    

    @staticmethod    
    @inline
    def comparegt5_am(other2: List[Vec[i16, 8]], other3: List[Vec[i16, 8]], other4: int, other5: List[Vec[i16, 8]], other6: List[Vec[i16, 8]], other7: List[Vec[i16, 8]], other8: List[Vec[i16, 8]], other9: Vec[i16, 8], other10: Vec[i16, 8], other11: List[Vec[i16, 8]], other12: List[Vec[i16, 8]], other13: Vec[u8, 8], other14: List[Vec[u8, 8]], other15: Vec[i16, 8], other16: Vec[i16, 8], other17: Vec[i16, 8], other18: Vec[u8, 8], other19: List[Vec[i16, 8]]):
        Vec._mm128_cmpgt_epi82_am(other2.arr.ptr, other3.arr.ptr, other4, other5.arr.ptr, other6.arr.ptr, other7.arr.ptr, other8.arr.ptr, other9, other10, other11.arr.ptr, other12.arr.ptr, other13, other14.arr.ptr, other15, other16, other4 + 1, other17, other18, other19.arr.ptr)

    @pure
    @derives
    @staticmethod
    @inline
    @llvm
    def _mm128_cmpgt_epi8final_am(m3: ptr, m: ptr, index: int, v1: ptr, v2: ptr, h1: ptr, h2: ptr, o: Vec[i16, 8], e: Vec[i16, 8], v: ptr, h: ptr, x: Vec[u8, 8], y, a: Vec[i16, 8], b: Vec[i16, 8], am: Vec[i16, 8], n: Vec[u8, 8], ms: ptr) -> None:        
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        declare <8 x i16> @llvm.smax.v8i16(<8 x i16>, <8 x i16>)
        %0 = getelementptr <8 x i16>, ptr %m3, i64 %index
        %1 = getelementptr <8 x i16>, ptr %v1, i64 %index
        %2 = getelementptr <8 x i16>, ptr %v2, i64 %index
        %3 = getelementptr <8 x i16>, ptr %h1, i64 %index
        %4 = getelementptr <8 x i16>, ptr %h2, i64 %index
        %5 = getelementptr <8 x i8>, ptr %y, i64 %index
        %6 = getelementptr <8 x i16>, ptr %m, i64 %index
        %7 = getelementptr <8 x i16>, ptr %v, i64 %index
        %8 = getelementptr <8 x i16>, ptr %h, i64 %index  
        call void @llvm.prefetch(ptr %6, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %7, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %8, i32 1, i32 3, i32 1)         
        %9 = load <8 x i16>, ptr %0
        %10 = load <8 x i16>, ptr %1
        %11 = load <8 x i16>, ptr %2
        %12 = load <8 x i16>, ptr %3
        %13 = load <8 x i16>, ptr %4
        %14 = load <8 x i8>, ptr %5
        %15 = icmp eq <8 x i8> %n, %x
        %16 = zext <8 x i1> %15 to <8 x i16>
        %17 = icmp eq <8 x i8> %n, %14
        %18 = zext <8 x i1> %17 to <8 x i16>
        %19 = or <8 x i16> %16, %18
        %20 = xor <8 x i16> %19, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %21 = mul <8 x i16> %19, %am
        %22 = icmp eq <8 x i8> %x, %14
        %23 = zext <8 x i1> %22 to <8 x i16>
        %24 = mul <8 x i16> %23, %a
        %25 = xor <8 x i16> %23, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %26 = mul <8 x i16> %25, %b
        %27 = add <8 x i16> %24, %26
        %28 = mul <8 x i16> %27, %20
        %29 = add <8 x i16> %28, %21
        %30 = add <8 x i16> %29, %9
        %31 = add <8 x i16> %11, %o
        %32 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %31, <8 x i16> %10)
        %33 = add <8 x i16> %32, %e
        %34 = add <8 x i16> %13, %o
        %35 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %34, <8 x i16> %12)
        %36 = add <8 x i16> %35, %e
        %37 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %33, <8 x i16> %36)
        %38 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %37, <8 x i16> %30)
        store <8 x i16> %38, ptr %6
        store <8 x i16> %33, ptr %7
        store <8 x i16> %36, ptr %8
        %39 = getelementptr <8 x i16>, ptr %ms, i64 %index
        %40 = load <8 x i16>, ptr %39
        %41 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %38, <8 x i16> %40)
        store <8 x i16> %41, ptr %39
        ret {} {}

    @staticmethod    
    @inline
    def comparegtfinal_am(other2: List[Vec[i16, 8]], other3: List[Vec[i16, 8]], other4: int, other5: List[Vec[i16, 8]], other6: List[Vec[i16, 8]], other7: List[Vec[i16, 8]], other8: List[Vec[i16, 8]], other9: Vec[i16, 8], other10: Vec[i16, 8], other11: List[Vec[i16, 8]], other12: List[Vec[i16, 8]], other13: Vec[u8, 8], other14: List[Vec[u8, 8]], other15: Vec[i16, 8], other16: Vec[i16, 8], other17: Vec[i16, 8], other18: Vec[u8, 8], other19: List[Vec[i16, 8]]):
        Vec._mm128_cmpgt_epi8final_am(other2.arr.ptr, other3.arr.ptr, other4, other5.arr.ptr, other6.arr.ptr, other7.arr.ptr, other8.arr.ptr, other9, other10, other11.arr.ptr, other12.arr.ptr, other13, other14.arr.ptr, other15, other16, other17, other18, other19.arr.ptr)        

    @staticmethod
    @inline
    @llvm
    def _mm128_cmpgt_epi82(m3: ptr, m: ptr, index: int, v1: ptr, v2: ptr, h1: ptr, h2: ptr, o: Vec[i16, 8], e: Vec[i16, 8], v: ptr, h: ptr, x: Vec[u8, 8], y, a: Vec[i16, 8], b: Vec[i16, 8], index2: int) -> None:        
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        declare <8 x i16> @llvm.smax.v8i16(<8 x i16>, <8 x i16>)
        %0 = getelementptr <8 x i16>, ptr %m3, i64 %index
        %1 = getelementptr <8 x i16>, ptr %v1, i64 %index
        %2 = getelementptr <8 x i16>, ptr %v2, i64 %index
        %3 = getelementptr <8 x i16>, ptr %h1, i64 %index
        %4 = getelementptr <8 x i16>, ptr %h2, i64 %index
        %5 = getelementptr <8 x i8>, ptr %y, i64 %index
        %6 = getelementptr <8 x i16>, ptr %m, i64 %index
        %7 = getelementptr <8 x i16>, ptr %v, i64 %index
        %8 = getelementptr <8 x i16>, ptr %h, i64 %index  
        call void @llvm.prefetch(ptr %6, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %7, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %8, i32 1, i32 3, i32 1)         
        %9 = load <8 x i16>, ptr %0
        %10 = load <8 x i16>, ptr %1
        %11 = load <8 x i16>, ptr %2
        %12 = load <8 x i16>, ptr %3
        %13 = load <8 x i16>, ptr %4
        %14 = load <8 x i8>, ptr %5
        %15 = icmp eq <8 x i8> %x, %14
        %16 = zext <8 x i1> %15 to <8 x i16>
        %17 = mul <8 x i16> %16, %a
        %18 = xor <8 x i16> %16, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %19 = mul <8 x i16> %18, %b
        %20 = add <8 x i16> %17, %19
        %21 = add <8 x i16> %20, %9
        %22 = add <8 x i16> %11, %o
        %23 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %22, <8 x i16> %10)
        %24 = add <8 x i16> %23, %e
        %25 = add <8 x i16> %13, %o
        %26 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %25, <8 x i16> %12)
        %27 = add <8 x i16> %26, %e
        %28 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %24, <8 x i16> %27)
        %29 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %28, <8 x i16> %21)
        store <8 x i16> %29, ptr %6
        store <8 x i16> %24, ptr %7
        store <8 x i16> %27, ptr %8
        %30 = getelementptr <8 x i16>, ptr %m3, i64 %index2
        %31 = getelementptr <8 x i16>, ptr %v1, i64 %index2
        %32 = getelementptr <8 x i16>, ptr %v2, i64 %index2
        %33 = getelementptr <8 x i16>, ptr %h1, i64 %index2
        %34 = getelementptr <8 x i16>, ptr %h2, i64 %index2
        %35 = getelementptr <8 x i8>, ptr %y, i64 %index2
        %36 = getelementptr <8 x i16>, ptr %m, i64 %index2
        %37 = getelementptr <8 x i16>, ptr %v, i64 %index2
        %38 = getelementptr <8 x i16>, ptr %h, i64 %index2 
        call void @llvm.prefetch(ptr %36, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %37, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %38, i32 1, i32 3, i32 1)         
        %39 = load <8 x i16>, ptr %30
        %40 = load <8 x i16>, ptr %31
        %41 = load <8 x i16>, ptr %32
        %42 = load <8 x i16>, ptr %33
        %43 = load <8 x i16>, ptr %34
        %44 = load <8 x i8>, ptr %35
        %45 = icmp eq <8 x i8> %x, %44
        %46 = zext <8 x i1> %45 to <8 x i16>
        %47 = mul <8 x i16> %46, %a
        %48 = xor <8 x i16> %46, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %49 = mul <8 x i16> %48, %b
        %50 = add <8 x i16> %47, %49
        %51 = add <8 x i16> %50, %39
        %52 = add <8 x i16> %41, %o
        %53 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %52, <8 x i16> %40)
        %54 = add <8 x i16> %53, %e
        %55 = add <8 x i16> %43, %o
        %56 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %55, <8 x i16> %42)
        %57 = add <8 x i16> %56, %e
        %58 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %54, <8 x i16> %57)
        %59 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %58, <8 x i16> %51)
        store <8 x i16> %59, ptr %36
        store <8 x i16> %54, ptr %37
        store <8 x i16> %57, ptr %38        
        ret {} {}
    

    @staticmethod    
    @inline
    def comparegt5(other2: List[Vec[i16, 8]], other3: List[Vec[i16, 8]], other4: int, other5: List[Vec[i16, 8]], other6: List[Vec[i16, 8]], other7: List[Vec[i16, 8]], other8: List[Vec[i16, 8]], other9: Vec[i16, 8], other10: Vec[i16, 8], other11: List[Vec[i16, 8]], other12: List[Vec[i16, 8]], other13: Vec[u8, 8], other14: List[Vec[u8, 8]], other15: Vec[i16, 8], other16: Vec[i16, 8]):
        Vec._mm128_cmpgt_epi82(other2.arr.ptr, other3.arr.ptr, other4, other5.arr.ptr, other6.arr.ptr, other7.arr.ptr, other8.arr.ptr, other9, other10, other11.arr.ptr, other12.arr.ptr, other13, other14.arr.ptr, other15, other16, other4 + 1)

    @pure
    @derives
    @staticmethod
    @inline
    @llvm
    def _mm128_cmpgt_epi8final(m3: ptr, m: ptr, index: int, v1: ptr, v2: ptr, h1: ptr, h2: ptr, o: Vec[i16, 8], e: Vec[i16, 8], v: ptr, h: ptr, x: Vec[u8, 8], y, a: Vec[i16, 8], b: Vec[i16, 8]) -> None:        
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        declare <8 x i16> @llvm.smax.v8i16(<8 x i16>, <8 x i16>)
        %0 = getelementptr <8 x i16>, ptr %m3, i64 %index
        %1 = getelementptr <8 x i16>, ptr %v1, i64 %index
        %2 = getelementptr <8 x i16>, ptr %v2, i64 %index
        %3 = getelementptr <8 x i16>, ptr %h1, i64 %index
        %4 = getelementptr <8 x i16>, ptr %h2, i64 %index
        %5 = getelementptr <8 x i8>, ptr %y, i64 %index
        %6 = getelementptr <8 x i16>, ptr %m, i64 %index
        %7 = getelementptr <8 x i16>, ptr %v, i64 %index
        %8 = getelementptr <8 x i16>, ptr %h, i64 %index  
        call void @llvm.prefetch(ptr %6, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %7, i32 1, i32 3, i32 1) 
        call void @llvm.prefetch(ptr %8, i32 1, i32 3, i32 1)         
        %9 = load <8 x i16>, ptr %0
        %10 = load <8 x i16>, ptr %1
        %11 = load <8 x i16>, ptr %2
        %12 = load <8 x i16>, ptr %3
        %13 = load <8 x i16>, ptr %4
        %14 = load <8 x i8>, ptr %5
        %15 = icmp eq <8 x i8> %x, %14
        %16 = zext <8 x i1> %15 to <8 x i16>
        %17 = mul <8 x i16> %16, %a
        %18 = xor <8 x i16> %16, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %19 = mul <8 x i16> %18, %b
        %20 = add <8 x i16> %17, %19
        %21 = add <8 x i16> %20, %9
        %22 = add <8 x i16> %11, %o
        %23 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %22, <8 x i16> %10)
        %24 = add <8 x i16> %23, %e
        %25 = add <8 x i16> %13, %o
        %26 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %25, <8 x i16> %12)
        %27 = add <8 x i16> %26, %e
        %28 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %24, <8 x i16> %27)
        %29 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %28, <8 x i16> %21)
        store <8 x i16> %29, ptr %6
        store <8 x i16> %24, ptr %7
        store <8 x i16> %27, ptr %8
        ret {} {}

    @staticmethod    
    @inline
    def comparegtfinal(other2: List[Vec[i16, 8]], other3: List[Vec[i16, 8]], other4: int, other5: List[Vec[i16, 8]], other6: List[Vec[i16, 8]], other7: List[Vec[i16, 8]], other8: List[Vec[i16, 8]], other9: Vec[i16, 8], other10: Vec[i16, 8], other11: List[Vec[i16, 8]], other12: List[Vec[i16, 8]], other13: Vec[u8, 8], other14: List[Vec[u8, 8]], other15: Vec[i16, 8], other16: Vec[i16, 8]):
        Vec._mm128_cmpgt_epi8final(other2.arr.ptr, other3.arr.ptr, other4, other5.arr.ptr, other6.arr.ptr, other7.arr.ptr, other8.arr.ptr, other9, other10, other11.arr.ptr, other12.arr.ptr, other13, other14.arr.ptr, other15, other16)        


    @pure
    @derives
    @staticmethod
    @inline
    @llvm
    def _mm256_cmpgt_vec(x: Vec[u8, 16], y: Vec[u8, 16], a: Vec[i16, 16], b: Vec[i16, 16], am: Vec[i16, 16], n: Vec[u8, 16]) -> Vec[i16, 16]:
        %0 = icmp eq <16 x i8> %n, %x
        %1 = icmp eq <16 x i8> %n, %y
        %2 = or <16 x i1> %0, %1
        %3 = zext <16 x i1> %2 to <16 x i16>
        %4 = xor <16 x i16> %3, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %5 = mul <16 x i16> %3, %am
        %6 = icmp eq <16 x i8> %x, %y
        %7 = zext <16 x i1> %6 to <16 x i16>
        %8 = mul <16 x i16> %7, %a
        %9 = xor <16 x i16> %7, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %10 = mul <16 x i16> %9, %b
        %11 = add <16 x i16> %8, %10
        %12 = mul <16 x i16> %11, %4
        %13 = add <16 x i16> %12, %5
        ret <16 x i16> %13

    @pure
    @derives
    @staticmethod
    @inline
    @llvm
    def _mm256_cmpgt_vec_non(x: Vec[u8, 16], y: Vec[u8, 16], a: Vec[i16, 16], b: Vec[i16, 16]) -> Vec[i16, 16]:
        %0 = icmp eq <16 x i8> %x, %y
        %1 = zext <16 x i1> %0 to <16 x i16>
        %2 = mul <16 x i16> %1, %a
        %3 = xor <16 x i16> %1, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %4 = mul <16 x i16> %3, %b
        %5 = add <16 x i16> %2, %4
        ret <16 x i16> %5

    @staticmethod    
    @inline
    def cmp_vec(other2: Vec[u8, 16], other3: Vec[u8, 16], other4: Vec[i16, 16], other5: Vec[i16, 16], other6: Vec[i16, 16], other7: Vec[u8, 16]) -> Vec[i16, 16]:
        if other6[0] != i16(0):
            return Vec._mm256_cmpgt_vec(other2, other3, other4, other5, other6, other7)   
        else:
            return Vec._mm256_cmpgt_vec_non(other2, other3, other4, other5)   

    @pure
    @derives
    @staticmethod
    @inline
    @llvm
    def _mm256_cmpgt_vec_2(x: Vec[u8, 16], y: Vec[u8, 16], a: Vec[i16, 16], b: Vec[i16, 16]) -> Vec[i16, 16]:
        %0= icmp eq <16 x i8> %x, %y
        %1= zext <16 x i1> %0 to <16 x i16>
        %2= mul <16 x i16> %1, %a
        %3= xor <16 x i16> %1, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %4= mul <16 x i16> %3, %b
        %5= add <16 x i16> %2, %4
        ret <16 x i16> %5

    @staticmethod    
    @inline
    def cmp_vec_2(other2: Vec[u8, 16], other3: Vec[u8, 16], other4: Vec[i16, 16], other5: Vec[i16, 16]) -> Vec[i16, 16]:
        return Vec._mm256_cmpgt_vec_2(other2, other3, other4, other5)     


    @pure
    @derives
    @staticmethod
    @inline
    @llvm
    def _mm512_cmpgt_vec_f32(x: Vec[u8, 16], y: Vec[u8, 16], a: Vec[f32, 16], b: Vec[f32, 16], am: Vec[f32, 16], n: Vec[u8, 16]) -> Vec[f32, 16]:
        %0 = icmp eq <16 x i8> %n, %x
        %1 = icmp eq <16 x i8> %n, %y
        %2 = or <16 x i1> %0, %1
        %3 = zext <16 x i1> %2 to <16 x f32>
        %4 = xor <16 x f32> %3, <f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1>
        %5 = mul <16 x f32> %3, %am
        %6 = icmp eq <16 x i8> %x, %y
        %7 = zext <16 x i1> %6 to <16 x f32>
        %8 = mul <16 x f32> %7, %a
        %9 = xor <16 x f32> %7, <f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1>
        %10 = mul <16 x f32> %9, %b
        %11 = add <16 x f32> %8, %10
        %12 = mul <16 x f32> %11, %4
        %13 = add <16 x f32> %12, %5
        ret <16 x f32> %13

    @staticmethod    
    @inline
    def cmp_vec_f32(other2: Vec[u8, 16], other3: Vec[u8, 16], other4: Vec[f32, 16], other5: Vec[f32, 16], other6: Vec[f32, 16], other7: Vec[u8, 16]) -> Vec[f32, 16]:
        return Vec._mm512_cmpgt_vec_f32(other2, other3, other4, other5, other6, other7)   

    @pure
    @derives
    @staticmethod
    @inline
    @llvm
    def _mm512_cmpgt_vec_2_f32(x: Vec[u8, 16], y: Vec[u8, 16], a: Vec[f32, 16], b: Vec[f32, 16]) -> Vec[f32, 16]:
        %0= icmp eq <16 x i8> %x, %y
        %1= zext <16 x i1> %0 to <16 x f32>
        %2= mul <16 x f32> %1, %a
        %3= xor <16 x f32> %1, <f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1>
        %4= mul <16 x f32> %3, %b
        %5= add <16 x f32> %2, %4
        ret <16 x f32> %5

    @staticmethod    
    @inline
    def cmp_vec_2_f32(other2: Vec[u8, 16], other3: Vec[u8, 16], other4: Vec[f32, 16], other5: Vec[f32, 16]) -> Vec[f32, 16]:
        return Vec._mm512_cmpgt_vec_2_f32(other2, other3, other4, other5)                        


    @pure
    @derives
    @staticmethod
    @inline
    @llvm
    def _mm256_msv(l1: ptr, index: int, l2: ptr, a2: Vec[i16, 16], l3: ptr, a3: Vec[i16, 16]) -> Vec[i16, 16]:        
        declare <16 x i16> @llvm.smax.v16i16(<16 x i16>, <16 x i16>)
        %0 = getelementptr <16 x i16>, ptr %l1, i64 %index
        %1 = getelementptr <16 x i16>, ptr %l2, i64 %index
        %2 = getelementptr <16 x i16>, ptr %l3, i64 %index 
        %3 = load <16 x i16>, ptr %1
        %4 = load <16 x i16>, ptr %2
        %5 = add <16 x i16> %a2, %3
        %6 = add <16 x i16> %a3, %4
        %7 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %5, <16 x i16> %6)
        store <16 x i16> %7, ptr %0
        ret <16 x i16> %7

    @staticmethod    
    @inline
    def max_store_vec(other: List[Vec[i16, 16]], other2: int, other3: List[Vec[i16, 16]], other4: Vec[i16, 16], other5: List[Vec[i16, 16]], other6: Vec[i16, 16]) -> Vec[i16, 16]:
        return Vec._mm256_msv(other.arr.ptr, other2, other3.arr.ptr, other4, other5.arr.ptr, other6)            


    @staticmethod
    @inline
    @llvm
    def _mm_fetch256_epi16(lst, index: int) -> List[Vec[i16, 16]]:
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        %0 = getelementptr ptr, ptr %lst, i64 %index
        %1 = load ptr, ptr %0
        call void @llvm.prefetch(ptr %1, i32 1, i32 3, i32 1) 
        ret ptr %1

    @staticmethod
    @inline
    def fetch(self: List[List[Vec[i16, 16]]], other: int) -> List[Vec[i16, 16]]:
        return Vec._mm_fetch256_epi16(self.arr.ptr, other)  

    @llvm
    def _mm128_cmpgt_epi32(x: Vec[i32, 4], y: Vec[i32, 4]) -> Vec[i32, 4]:
        %0 = icmp sgt <4 x i32> %x, %y
        %1 = sext <4 x i1> %0 to <4 x i32>
        ret <4 x i32> %1

    def comparegt(self: Vec[i32, 4], other: Vec[i32, 4]) -> Vec[i32, 4]:
        return Vec._mm128_cmpgt_epi32(self, other)                    

    @llvm
    def _mm512_mul_epi32(x: Vec[i32, 16], y: Vec[i32, 16]) -> Vec[i32, 16]:
        %0 = mul <16 x i32> %x, %y        
        ret <16 x i32> %0

    def __mul__(self: Vec[i32, 16], other: Vec[i32, 16]) -> Vec[i32, 16]:
        return Vec._mm512_mul_epi32(self, other)  

    @llvm
    def _mm256_mul_epi16(x: Vec[i16, 16], y: Vec[i16, 16]) -> Vec[i16, 16]:
        %0 = mul <16 x i16> %x, %y        
        ret <16 x i16> %0

    def __mul__(self: Vec[i16, 16], other: Vec[i16, 16]) -> Vec[i16, 16]:
        return Vec._mm256_mul_epi16(self, other)    
        
    @llvm
    def _mm128_mul_epi16(x: Vec[i16, 8], y: Vec[i16, 8]) -> Vec[i16, 8]:
        %0 = mul <8 x i16> %x, %y        
        ret <8 x i16> %0

    def __mul__(self: Vec[i16, 8], other: Vec[i16, 8]) -> Vec[i16, 8]:
        return Vec._mm128_mul_epi16(self, other)   

    @llvm
    def _mm128_mul_epi16_scalar(x: Vec[i16, 8], y: i16) -> Vec[i16, 8]:
        %0 = insertelement <8 x i16> undef, i16 %y, i16 0
        %1 = shufflevector <8 x i16> %0, <8 x i16> undef, <8 x i32> zeroinitializer
        %2 = mul <8 x i16> %x, %1
        ret <8 x i16> %2

    def __mul__(self: Vec[i16, 8], other: i16) -> Vec[i16, 8]:
        return Vec._mm128_mul_epi16_scalar(self, other)     


    @llvm
    def _mm1024_mul_epi16(x: Vec[i16, 64], y: Vec[i16, 64]) -> Vec[i16, 64]:
        %0 = mul <64 x i16> %x, %y        
        ret <64 x i16> %0

    def __mul__(self: Vec[i16, 64], other: Vec[i16, 64]) -> Vec[i16, 64]:
        return Vec._mm1024_mul_epi16(self, other)  

    @llvm
    def _mm512_mul_epi16(x: Vec[i16, 32], y: Vec[i16, 32]) -> Vec[i16, 32]:
        %0 = mul <32 x i16> %x, %y        
        ret <32 x i16> %0

    def __mul__(self: Vec[i16, 32], other: Vec[i16, 32]) -> Vec[i16, 32]:
        return Vec._mm512_mul_epi16(self, other) 

    @llvm
    def _mm512_mul_epi16_scalar(x: Vec[i16, 32], y: i16) -> Vec[i16, 32]:
        %0 = insertelement <32 x i16> undef, i16 %y, i16 0
        %1 = shufflevector <32 x i16> %0, <32 x i16> undef, <32 x i32> zeroinitializer
        %2 = mul <32 x i16> %x, %1
        ret <32 x i16> %2

    def __mul__(self: Vec[i16, 32], other: i16) -> Vec[i16, 32]:
        return Vec._mm512_mul_epi16_scalar(self, other)    

    @llvm
    def _mm256_mul_epi16_scalar(x: Vec[i16, 16], y: i16) -> Vec[i16, 16]:
        %0 = insertelement <16 x i16> undef, i16 %y, i16 0
        %1 = shufflevector <16 x i16> %0, <16 x i16> undef, <16 x i32> zeroinitializer
        %2 = mul <16 x i16> %x, %1
        ret <16 x i16> %2

    def __mul__(self: Vec[i16, 16], other: i16) -> Vec[i16, 16]:
        return Vec._mm256_mul_epi16_scalar(self, other)   

    def __mul__(self: i16, other: Vec[i16, 16]) -> Vec[i16, 16]:
        return Vec._mm256_mul_epi16_scalar(other, self)

    @llvm
    def _mm128_mul_epi32(x: Vec[i32, 4], y: Vec[i32, 4]) -> Vec[i32, 4]:
        %0 = mul <4 x i32> %x, %y        
        ret <4 x i32> %0

    @pure
    @derives    
    @staticmethod
    @inline
    @llvm
    def __mm256_max_f32(x: Vec[f32, 8], y: Vec[f32, 8]) -> Vec[f32, 8]:
        declare <8 x f32> @llvm.fmax.v8f32(<8 x f32>, <8 x f32>)  
        %0 = call <8 x f32> @llvm.fmax.v8f32(<8 x f32> %x, <8 x f32> %y)
        ret <8 x f32> %0          

    @pure
    @derives    
    @staticmethod
    @inline    
    def max(self: Vec[f32, 8], other: Vec[f32, 8]) -> Vec[f32, 8]:
        return Vec.__mm256_max_f32(self, other)

    @pure
    @derives    
    @staticmethod
    @inline
    @llvm
    def __mm256_min_f32(x: Vec[f32, 8], y: Vec[f32, 8]) -> Vec[f32, 8]:
        declare <8 x f32> @llvm.fmin.v8f32(<8 x f32>, <8 x f32>)  
        %0 = call <8 x f32> @llvm.fmin.v8f32(<8 x f32> %x, <8 x f32> %y)
        ret <8 x f32> %0          

    @pure
    @derives    
    @staticmethod
    @inline    
    def min(self: Vec[f32, 8], other: Vec[f32, 8]) -> Vec[f32, 8]:
        return Vec.__mm256_min_f32(self, other)

    @pure
    @derives    
    @staticmethod
    @inline
    @llvm
    def __mm256_max_f32(x: Vec[f32, 8], y: Vec[f32, 8], z: Vec[f32, 8]) -> Vec[f32, 8]:
        declare <8 x f32> @llvm.fmax.v8f32(<8 x f32>, <8 x f32>)  
        %0 = call <8 x f32> @llvm.fmax.v8f32(<8 x f32> %x, <8 x f32> %y)
        %1 = call <8 x f32> @llvm.fmax.v8f32(<8 x f32> %0, <8 x f32> %z)
        ret <8 x f32> %1         

    @pure
    @derives    
    @staticmethod
    @inline
    def max(self: Vec[f32, 8], other: Vec[f32, 8], other2: Vec[f32, 8]) -> Vec[f32, 8]:
        return Vec.__mm256_max_f32(self, other, other2)

    @pure
    @derives    
    @staticmethod
    @inline
    @llvm
    def __mm128_max_i16(x: Vec[i16, 8], y: Vec[i16, 8], z: Vec[i16, 8]) -> Vec[i16, 8]:
        declare <8 x i16> @llvm.smax.v8i16(<8 x i16>, <8 x i16>)  
        %0 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %x, <8 x i16> %y)
        %1 = call <8 x i16> @llvm.smax.v8i16(<8 x i16> %0, <8 x i16> %z)
        ret <8 x i16> %1

    
    @pure
    @derives    
    @staticmethod
    @inline
    def max(self: Vec[i16, 8], other: Vec[i16, 8], other2: Vec[i16, 8]) -> Vec[i16, 8]:
        return Vec.__mm128_max_i16(self, other, other2)

    @pure
    @derives    
    @staticmethod
    @inline
    @llvm
    def __mm128_min_i16(x: Vec[i16, 8], y: Vec[i16, 8], z: Vec[i16, 8]) -> Vec[i16, 8]:
        declare <8 x i16> @llvm.smin.v8i16(<8 x i16>, <8 x i16>)  
        %0 = call <8 x i16> @llvm.smin.v8i16(<8 x i16> %x, <8 x i16> %y)
        %1 = call <8 x i16> @llvm.smin.v8i16(<8 x i16> %0, <8 x i16> %z)
        ret <8 x i16> %1
    
    @inline
    def min(self: Vec[i16, 8], other: Vec[i16, 8], other2: Vec[i16, 8]) -> Vec[i16, 8]:
        return Vec.__mm128_min_i16(self, other, other2)    
 
    
    @inline
    @llvm
    def __mm256_max_i16(x: Vec[i16, 16], y: Vec[i16, 16], z: Vec[i16, 16], ms: ptr, index: int, m: ptr) -> None:
        declare <16 x i16> @llvm.smax.v16i16(<16 x i16>, <16 x i16>)
        %0 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %x, <16 x i16> %y)
        %1 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %0, <16 x i16> %z)
        %2 = getelementptr <16 x i16>, ptr %ms, i64 %index
        %3 = load <16 x i16>, ptr %2
        %4 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %1, <16 x i16> %3)
        store <16 x i16> %4, ptr %2
        %5 = getelementptr <16 x i16>, ptr %m, i64 %index
        store <16 x i16> %1, ptr %5
        ret {} {}

      
    @inline
    def max(self: Vec[i16, 16], other: Vec[i16, 16], other2: Vec[i16, 16], other3: List[Vec[i16, 16]], other4: int, other5: List[Vec[i16, 16]]):        
        Vec.__mm256_max_i16(self, other, other2, other3.arr.ptr, other4, other5.arr.ptr)

    @inline
    @llvm
    def __mm256_max_i16_2(x: Vec[i16, 16], y: Vec[i16, 16], z: Vec[i16, 16], ms: ptr, index: int, m: ptr, x2: Vec[i16, 16], y2: Vec[i16, 16], z2: Vec[i16, 16], index2: int) -> None:
        declare <16 x i16> @llvm.smax.v16i16(<16 x i16>, <16 x i16>)
        %0 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %x, <16 x i16> %y)
        %1 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %0, <16 x i16> %z)
        %2 = getelementptr <16 x i16>, ptr %ms, i64 %index
        %3 = load <16 x i16>, ptr %2
        %4 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %1, <16 x i16> %3)
        store <16 x i16> %4, ptr %2
        %5 = getelementptr <16 x i16>, ptr %m, i64 %index
        store <16 x i16> %1, ptr %5
        %6 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %x2, <16 x i16> %y2)
        %7 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %6, <16 x i16> %z2)
        %8 = getelementptr <16 x i16>, ptr %ms, i64 %index2
        %9 = load <16 x i16>, ptr %8
        %10 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %7, <16 x i16> %9)
        store <16 x i16> %10, ptr %8
        %11 = getelementptr <16 x i16>, ptr %m, i64 %index2
        store <16 x i16> %7, ptr %11
        ret {} {}

      
    @inline
    def max_2(self: Vec[i16, 16], other: Vec[i16, 16], other2: Vec[i16, 16], other3: List[Vec[i16, 16]], other4: int, other5: List[Vec[i16, 16]], other6: Vec[i16, 16], other7: Vec[i16, 16], other8: Vec[i16, 16]):        
        Vec.__mm256_max_i16_2(self, other, other2, other3.arr.ptr, other4, other5.arr.ptr, other6, other7, other8, other4 + 1)        


    @inline
    @llvm
    def __mm512_max_f32_2(x: Vec[f32, 16], y: Vec[f32, 16], z: Vec[f32, 16], ms: ptr, index: int, m: ptr, x2: Vec[f32, 16], y2: Vec[f32, 16], z2: Vec[f32, 16], index2: int) -> None:
        declare <16 x f32> @llvm.smax.v16f32(<16 x f32>, <16 x f32>)
        %0 = call <16 x f32> @llvm.smax.v16f32(<16 x f32> %x, <16 x f32> %y)
        %1 = call <16 x f32> @llvm.smax.v16f32(<16 x f32> %0, <16 x f32> %z)
        %2 = getelementptr <16 x f32>, ptr %ms, i64 %index
        %3 = load <16 x f32>, ptr %2
        %4 = call <16 x f32> @llvm.smax.v16f32(<16 x f32> %1, <16 x f32> %3)
        store <16 x f32> %4, ptr %2
        %5 = getelementptr <16 x f32>, ptr %m, i64 %index
        store <16 x f32> %1, ptr %5
        %6 = call <16 x f32> @llvm.smax.v16f32(<16 x f32> %x2, <16 x f32> %y2)
        %7 = call <16 x f32> @llvm.smax.v16f32(<16 x f32> %6, <16 x f32> %z2)
        %8 = getelementptr <16 x f32>, ptr %ms, i64 %index2
        %9 = load <16 x f32>, ptr %8
        %10 = call <16 x f32> @llvm.smax.v16f32(<16 x f32> %7, <16 x f32> %9)
        store <16 x f32> %10, ptr %8
        %11 = getelementptr <16 x f32>, ptr %m, i64 %index2
        store <16 x f32> %7, ptr %11
        ret {} {}

      
    @inline
    def max_2_f32(self: Vec[f32, 16], other: Vec[f32, 16], other2: Vec[f32, 16], other3: List[Vec[f32, 16]], other4: int, other5: List[Vec[f32, 16]], other6: Vec[f32, 16], other7: Vec[f32, 16], other8: Vec[f32, 16]):        
        Vec.__mm512_max_f32_2(self, other, other2, other3.arr.ptr, other4, other5.arr.ptr, other6, other7, other8, other4 + 1)        

    @inline
    @llvm
    def __mm256_max_i16_4(x: Vec[i16, 16], y: Vec[i16, 16], z: Vec[i16, 16], ms: ptr, index: int, m: ptr, x2: Vec[i16, 16], y2: Vec[i16, 16], z2: Vec[i16, 16], index2: int, x3: Vec[i16, 16], y3: Vec[i16, 16], z3: Vec[i16, 16], index3: int, x4: Vec[i16, 16], y4: Vec[i16, 16], z4: Vec[i16, 16], index4: int) -> None:
        declare <16 x i16> @llvm.smax.v16i16(<16 x i16>, <16 x i16>)
        %0 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %x, <16 x i16> %y)
        %1 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %0, <16 x i16> %z)
        %2 = getelementptr <16 x i16>, ptr %ms, i64 %index
        %3 = load <16 x i16>, ptr %2
        %4 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %1, <16 x i16> %3)
        store <16 x i16> %4, ptr %2
        %5 = getelementptr <16 x i16>, ptr %m, i64 %index
        store <16 x i16> %1, ptr %5
        %6 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %x2, <16 x i16> %y2)
        %7 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %6, <16 x i16> %z2)
        %8 = getelementptr <16 x i16>, ptr %ms, i64 %index2
        %9 = load <16 x i16>, ptr %8
        %10 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %7, <16 x i16> %9)
        store <16 x i16> %10, ptr %8
        %11 = getelementptr <16 x i16>, ptr %m, i64 %index2
        store <16 x i16> %7, ptr %11
        %12= call <16 x i16> @llvm.smax.v16i16(<16 x i16> %x3, <16 x i16> %y3)
        %13= call <16 x i16> @llvm.smax.v16i16(<16 x i16> %12, <16 x i16> %z3)
        %14= getelementptr <16 x i16>, ptr %ms, i64 %index3
        %15= load <16 x i16>, ptr %14
        %16= call <16 x i16> @llvm.smax.v16i16(<16 x i16> %13, <16 x i16> %15)
        store <16 x i16> %16, ptr %14
        %17= getelementptr <16 x i16>, ptr %m, i64 %index3
        store <16 x i16> %13, ptr %17
        %18= call <16 x i16> @llvm.smax.v16i16(<16 x i16> %x4, <16 x i16> %y4)
        %19= call <16 x i16> @llvm.smax.v16i16(<16 x i16> %18, <16 x i16> %z4)
        %20= getelementptr <16 x i16>, ptr %ms, i64 %index4
        %21= load <16 x i16>, ptr %20
        %22= call <16 x i16> @llvm.smax.v16i16(<16 x i16> %19, <16 x i16> %21)
        store <16 x i16> %22, ptr %20
        %23= getelementptr <16 x i16>, ptr %m, i64 %index4
        store <16 x i16> %19, ptr %23
        ret {} {}

      
    @inline
    def max_4(self: Vec[i16, 16], other: Vec[i16, 16], other2: Vec[i16, 16], other3: List[Vec[i16, 16]], other4: int, other5: List[Vec[i16, 16]], other6: Vec[i16, 16], other7: Vec[i16, 16], other8: Vec[i16, 16], other9: Vec[i16, 16], other10: Vec[i16, 16], other11: Vec[i16, 16], other12: Vec[i16, 16], other13: Vec[i16, 16], other14: Vec[i16, 16]):        
        Vec.__mm256_max_i16_4(self, other, other2, other3.arr.ptr, other4, other5.arr.ptr, other6, other7, other8, other4 + 1, other9, other10, other11, other4 + 2, other12, other13, other14, other4 + 3)        

    @inline
    @llvm
    def __mm256_min_i16(x: Vec[i16, 16], y: Vec[i16, 16], z: Vec[i16, 16], ms: ptr, index: int, m: ptr) -> None:
        declare <16 x i16> @llvm.smin.v16i16(<16 x i16>, <16 x i16>)  
        declare <16 x i16> @llvm.smax.v16i16(<16 x i16>, <16 x i16>)
        %0 = call <16 x i16> @llvm.smin.v16i16(<16 x i16> %x, <16 x i16> %y)
        %1 = call <16 x i16> @llvm.smin.v16i16(<16 x i16> %0, <16 x i16> %z)
        %2 = getelementptr <16 x i16>, ptr %ms, i64 %index
        %3 = load <16 x i16>, ptr %2
        %4 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %1, <16 x i16> %3)
        store <16 x i16> %4, ptr %2        
        %5 = getelementptr <16 x i16>, ptr %m, i64 %index
        store <16 x i16> %1, ptr %5
        ret {} {}
     
    @inline
    def min(self: Vec[i16, 16], other: Vec[i16, 16], other2: Vec[i16, 16], other3: List[Vec[i16, 16]], other4: int, other5: List[Vec[i16, 16]]):
        Vec.__mm256_min_i16(self, other, other2, other3.arr.ptr, other4, other5.arr.ptr)      

    @inline
    @llvm
    def __mm256_min_i16_2(x: Vec[i16, 16], y: Vec[i16, 16], z: Vec[i16, 16], ms: ptr, index: int, m: ptr, x2: Vec[i16, 16], y2: Vec[i16, 16], z2: Vec[i16, 16], index2: int) -> None:
        declare <16 x i16> @llvm.smin.v16i16(<16 x i16>, <16 x i16>)
        declare <16 x i16> @llvm.smax.v16i16(<16 x i16>, <16 x i16>)
        %0 = call <16 x i16> @llvm.smin.v16i16(<16 x i16> %x, <16 x i16> %y)
        %1 = call <16 x i16> @llvm.smin.v16i16(<16 x i16> %0, <16 x i16> %z)
        %2 = getelementptr <16 x i16>, ptr %ms, i64 %index
        %3 = load <16 x i16>, ptr %2
        %4 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %1, <16 x i16> %3)
        store <16 x i16> %4, ptr %2
        %5 = getelementptr <16 x i16>, ptr %m, i64 %index
        store <16 x i16> %1, ptr %5
        %6 = call <16 x i16> @llvm.smin.v16i16(<16 x i16> %x2, <16 x i16> %y2)
        %7 = call <16 x i16> @llvm.smin.v16i16(<16 x i16> %6, <16 x i16> %z2)
        %8 = getelementptr <16 x i16>, ptr %ms, i64 %index2
        %9 = load <16 x i16>, ptr %8
        %10 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %7, <16 x i16> %9)
        store <16 x i16> %10, ptr %8
        %11 = getelementptr <16 x i16>, ptr %m, i64 %index2
        store <16 x i16> %7, ptr %11
        ret {} {}

      
    @inline
    def min_2(self: Vec[i16, 16], other: Vec[i16, 16], other2: Vec[i16, 16], other3: List[Vec[i16, 16]], other4: int, other5: List[Vec[i16, 16]], other6: Vec[i16, 16], other7: Vec[i16, 16], other8: Vec[i16, 16]):        
        Vec.__mm256_min_i16_2(self, other, other2, other3.arr.ptr, other4, other5.arr.ptr, other6, other7, other8, other4 + 1)

    @inline
    @llvm
    def __mm256_min_i16_4(x: Vec[i16, 16], y: Vec[i16, 16], z: Vec[i16, 16], ms: ptr, index: int, m: ptr, x2: Vec[i16, 16], y2: Vec[i16, 16], z2: Vec[i16, 16], index2: int, x3: Vec[i16, 16], y3: Vec[i16, 16], z3: Vec[i16, 16], index3: int, x4: Vec[i16, 16], y4: Vec[i16, 16], z4: Vec[i16, 16], index4: int) -> None:
        declare <16 x i16> @llvm.smin.v16i16(<16 x i16>, <16 x i16>)
        declare <16 x i16> @llvm.smax.v16i16(<16 x i16>, <16 x i16>)
        %0 = call <16 x i16> @llvm.smin.v16i16(<16 x i16> %x, <16 x i16> %y)
        %1 = call <16 x i16> @llvm.smin.v16i16(<16 x i16> %0, <16 x i16> %z)
        %2 = getelementptr <16 x i16>, ptr %ms, i64 %index
        %3 = load <16 x i16>, ptr %2
        %4 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %1, <16 x i16> %3)
        store <16 x i16> %4, ptr %2
        %5 = getelementptr <16 x i16>, ptr %m, i64 %index
        store <16 x i16> %1, ptr %5
        %6 = call <16 x i16> @llvm.smin.v16i16(<16 x i16> %x2, <16 x i16> %y2)
        %7 = call <16 x i16> @llvm.smin.v16i16(<16 x i16> %6, <16 x i16> %z2)
        %8 = getelementptr <16 x i16>, ptr %ms, i64 %index2
        %9 = load <16 x i16>, ptr %8
        %10 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %7, <16 x i16> %9)
        store <16 x i16> %10, ptr %8
        %11 = getelementptr <16 x i16>, ptr %m, i64 %index2
        store <16 x i16> %7, ptr %11
        %12= call <16 x i16> @llvm.smin.v16i16(<16 x i16> %x3, <16 x i16> %y3)
        %13= call <16 x i16> @llvm.smin.v16i16(<16 x i16> %12, <16 x i16> %z3)
        %14= getelementptr <16 x i16>, ptr %ms, i64 %index3
        %15= load <16 x i16>, ptr %14
        %16= call <16 x i16> @llvm.smax.v16i16(<16 x i16> %13, <16 x i16> %15)
        store <16 x i16> %16, ptr %14
        %17= getelementptr <16 x i16>, ptr %m, i64 %index3
        store <16 x i16> %13, ptr %17
        %18= call <16 x i16> @llvm.smin.v16i16(<16 x i16> %x4, <16 x i16> %y4)
        %19= call <16 x i16> @llvm.smin.v16i16(<16 x i16> %18, <16 x i16> %z4)
        %20= getelementptr <16 x i16>, ptr %ms, i64 %index4
        %21= load <16 x i16>, ptr %20
        %22= call <16 x i16> @llvm.smax.v16i16(<16 x i16> %19, <16 x i16> %21)
        store <16 x i16> %22, ptr %20
        %23= getelementptr <16 x i16>, ptr %m, i64 %index4
        store <16 x i16> %19, ptr %23
        ret {} {}

      
    @inline
    def min_4(self: Vec[i16, 16], other: Vec[i16, 16], other2: Vec[i16, 16], other3: List[Vec[i16, 16]], other4: int, other5: List[Vec[i16, 16]], other6: Vec[i16, 16], other7: Vec[i16, 16], other8: Vec[i16, 16], other9: Vec[i16, 16], other10: Vec[i16, 16], other11: Vec[i16, 16], other12: Vec[i16, 16], other13: Vec[i16, 16], other14: Vec[i16, 16]):        
        Vec.__mm256_min_i16_4(self, other, other2, other3.arr.ptr, other4, other5.arr.ptr, other6, other7, other8, other4 + 1, other9, other10, other11, other4 + 2, other12, other13, other14, other4 + 3)        


    @inline
    @llvm
    def __mm256_max_i162(x: Vec[i16, 16], y: Vec[i16, 16], ms: ptr, index: int, m: ptr) -> None:
        declare <16 x i16> @llvm.smax.v16i16(<16 x i16>, <16 x i16>)  
        %0 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %x, <16 x i16> %y)
        %1 = getelementptr <16 x i16>, ptr %ms, i64 %index
        %2 = load <16 x i16>, ptr %1
        %3 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %2, <16 x i16> %0) 
        store <16 x i16> %3, ptr %1   
        %4 = getelementptr <16 x i16>, ptr %m, i64 %index
        store <16 x i16> %0, ptr %4   
        ret {} {}

    
    @inline
    def max(self: Vec[i16, 16], other: Vec[i16, 16], other2: List[Vec[i16, 16]], other3: int, other4: List[Vec[i16, 16]]):
        Vec.__mm256_max_i162(self, other, other2.arr.ptr, other3, other4.arr.ptr)

    @inline
    @llvm
    def __mm256_max_i162_2(x: Vec[i16, 16], y: Vec[i16, 16], ms: ptr, index: int, m: ptr, x2: Vec[i16, 16], y2: Vec[i16, 16], index2: int) -> None:
        declare <16 x i16> @llvm.smax.v16i16(<16 x i16>, <16 x i16>)  
        %0 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %x, <16 x i16> %y)
        %1 = getelementptr <16 x i16>, ptr %ms, i64 %index
        %2 = load <16 x i16>, ptr %1
        %3 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %2, <16 x i16> %0) 
        store <16 x i16> %3, ptr %1   
        %4 = getelementptr <16 x i16>, ptr %m, i64 %index
        store <16 x i16> %0, ptr %4   
        %5= call <16 x i16> @llvm.smax.v16i16(<16 x i16> %x2, <16 x i16> %y2)
        %6= getelementptr <16 x i16>, ptr %ms, i64 %index2
        %7= load <16 x i16>, ptr %6
        %8= call <16 x i16> @llvm.smax.v16i16(<16 x i16> %7, <16 x i16> %5) 
        store <16 x i16> %8, ptr %6
        %9= getelementptr <16 x i16>, ptr %m, i64 %index2
        store <16 x i16> %5, ptr %9
        ret {} {}

    
    @inline
    def max_2(self: Vec[i16, 16], other: Vec[i16, 16], other2: List[Vec[i16, 16]], other3: int, other4: List[Vec[i16, 16]], other5: Vec[i16, 16], other6: Vec[i16, 16]):
        Vec.__mm256_max_i162_2(self, other, other2.arr.ptr, other3, other4.arr.ptr, other5, other6, other3 + 1)        

    @inline
    @llvm
    def __mm512_max_f322_2(x: Vec[f32, 16], y: Vec[f32, 16], ms: ptr, index: int, m: ptr, x2: Vec[f32, 16], y2: Vec[f32, 16], index2: int) -> None:
        declare <16 x f32> @llvm.smax.v16f32(<16 x f32>, <16 x f32>)  
        %0 = call <16 x f32> @llvm.smax.v16f32(<16 x f32> %x, <16 x f32> %y)
        %1 = getelementptr <16 x f32>, ptr %ms, i64 %index
        %2 = load <16 x f32>, ptr %1
        %3 = call <16 x f32> @llvm.smax.v16f32(<16 x f32> %2, <16 x f32> %0) 
        store <16 x f32> %3, ptr %1   
        %4 = getelementptr <16 x f32>, ptr %m, i64 %index
        store <16 x f32> %0, ptr %4   
        %5= call <16 x f32> @llvm.smax.v16f32(<16 x f32> %x2, <16 x f32> %y2)
        %6= getelementptr <16 x f32>, ptr %ms, i64 %index2
        %7= load <16 x f32>, ptr %6
        %8= call <16 x f32> @llvm.smax.v16f32(<16 x f32> %7, <16 x f32> %5) 
        store <16 x f32> %8, ptr %6
        %9= getelementptr <16 x f32>, ptr %m, i64 %index2
        store <16 x f32> %5, ptr %9
        ret {} {}

    
    @inline
    def max_2_f32(self: Vec[f32, 16], other: Vec[f32, 16], other2: List[Vec[f32, 16]], other3: int, other4: List[Vec[f32, 16]], other5: Vec[f32, 16], other6: Vec[f32, 16]):
        Vec.__mm512_max_f322_2(self, other, other2.arr.ptr, other3, other4.arr.ptr, other5, other6, other3 + 1)                

    @inline
    @llvm
    def __mm256_min_i162(x: Vec[i16, 16], y: Vec[i16, 16], ms: ptr, index: int, m: ptr) -> None:
        declare <16 x i16> @llvm.smin.v16i16(<16 x i16>, <16 x i16>)  
        declare <16 x i16> @llvm.smax.v16i16(<16 x i16>, <16 x i16>)
        %0 = call <16 x i16> @llvm.smin.v16i16(<16 x i16> %x, <16 x i16> %y)
        %1 = getelementptr <16 x i16>, ptr %ms, i64 %index
        %2 = load <16 x i16>, ptr %1
        %3 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %2, <16 x i16> %0)
        store <16 x i16> %3, ptr %1
        %4 = getelementptr <16 x i16>, ptr %m, i64 %index
        store <16 x i16> %0, ptr %4   
        ret {} {}
      
    @inline
    def min(self: Vec[i16, 16], other: Vec[i16, 16], other2: List[Vec[i16, 16]], other3: int, other4: List[Vec[i16, 16]]):
        Vec.__mm256_min_i162(self, other, other2.arr.ptr, other3, other4.arr.ptr)

    @inline
    @llvm
    def __mm256_min_i162_2(x: Vec[i16, 16], y: Vec[i16, 16], ms: ptr, index: int, m: ptr, x2: Vec[i16, 16], y2: Vec[i16, 16], index2: int) -> None:
        declare <16 x i16> @llvm.smin.v16i16(<16 x i16>, <16 x i16>)  
        declare <16 x i16> @llvm.smax.v16i16(<16 x i16>, <16 x i16>)
        %0 = call <16 x i16> @llvm.smin.v16i16(<16 x i16> %x, <16 x i16> %y)
        %1 = getelementptr <16 x i16>, ptr %ms, i64 %index
        %2 = load <16 x i16>, ptr %1
        %3 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %2, <16 x i16> %0)
        store <16 x i16> %3, ptr %1
        %4 = getelementptr <16 x i16>, ptr %m, i64 %index
        store <16 x i16> %0, ptr %4   
        %5 = call <16 x i16> @llvm.smin.v16i16(<16 x i16> %x2, <16 x i16> %y2)
        %6 = getelementptr <16 x i16>, ptr %ms, i64 %index2
        %7 = load <16 x i16>, ptr %6
        %8 = call <16 x i16> @llvm.smax.v16i16(<16 x i16> %7, <16 x i16> %5)
        store <16 x i16> %3, ptr %6
        %9 = getelementptr <16 x i16>, ptr %m, i64 %index2
        store <16 x i16> %5, ptr %9
        ret {} {}
      
    @inline
    def min_2(self: Vec[i16, 16], other: Vec[i16, 16], other2: List[Vec[i16, 16]], other3: int, other4: List[Vec[i16, 16]], other5: Vec[i16, 16], other6: Vec[i16, 16]):
        Vec.__mm256_min_i162_2(self, other, other2.arr.ptr, other3, other4.arr.ptr, other5, other6, other3 + 1)        

    @pure
    @derives    
    @staticmethod
    @inline
    @llvm
    def __mm256_min_f32(x: Vec[f32, 8], y: Vec[f32, 8], z: Vec[f32, 8]) -> Vec[f32, 8]:
        declare <8 x f32> @llvm.fmin.v8f32(<8 x f32>, <8 x f32>)  
        %0 = call <8 x f32> @llvm.fmin.v8f32(<8 x f32> %x, <8 x f32> %y)
        %1 = call <8 x f32> @llvm.fmin.v8f32(<8 x f32> %0, <8 x f32> %z)
        ret <8 x f32> %1     

    
    @pure
    @derives    
    @staticmethod
    @inline
    def min(self: Vec[f32, 8], other: Vec[f32, 8], other2: Vec[f32, 8]) -> Vec[f32, 8]:
        return Vec.__mm256_min_f32(self, other, other2)            

    def __mul__(self: Vec[i32, 4], other: Vec[i32, 4]) -> Vec[i32, 4]:
        return Vec._mm128_mul_epi32(self, other)  

    @llvm
    def _mm_bsrli_256(vec: Vec[u8, 32]) -> Vec[u8, 32]:
        %0 = shufflevector <32 x i8> %vec, <32 x i8> zeroinitializer, <32 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31, i32 32, i32 33, i32 34, i32 35, i32 36, i32 37, i32 38, i32 39, i32 40, i32 41, i32 42, i32 43, i32 44, i32 45, i32 46, i32 47>        
        ret <32 x i8> %0

    def __rshift__(self: Vec[u8, 32], shift: Static[int]) -> Vec[u8, 32]:
        if shift == 0:
            return self
        elif shift == 16:
            return Vec._mm_bsrli_256(self)
        else:
            compile_error("invalid bitshift")
  
    # @llvm  # https://stackoverflow.com/questions/6996764/fastest-way-to-do-horizontal-sse-vector-sum-or-other-reduction
    # def sum(self: Vec[f32, 8]) -> f32:
    #     %0 = shufflevector <8 x float> %self, <8 x float> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
    #     %1 = shufflevector <8 x float> %self, <8 x float> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
    #     %2 = fadd <4 x float> %0, %1
    #     %3 = shufflevector <4 x float> %2, <4 x float> undef, <4 x i32> <i32 1, i32 undef, i32 3, i32 undef>
    #     %4 = fadd <4 x float> %2, %3
    #     %5 = shufflevector <4 x float> %4, <4 x float> poison, <4 x i32> <i32 2, i32 undef, i32 undef, i32 undef>
    #     %6 = fadd <4 x float> %4, %5
    #     %7 = extractelement <4 x float> %6, i32 0
    #     ret float %7

    def sum(self: Vec[f32, 8], x: f32 = f32(0.0)) -> f32:
        return x + self[0] + self[1] + self[2] + self[3] + self[4] + self[5] + self[6] + self[7]       

    @llvm
    def __getitem__(self, n: Static[int]) -> T:
        %0 = extractelement <{=N} x {=T}> %self, i32 {=n}
        ret {=T} %0
    
    @llvm
    def __mm_cvt_epi162epi8(x: Vec[i16, 32]) -> Vec[u8, 32]:
        %0 = trunc <32 x i16> %x to <32 x i8>
        ret <32 x i8> %0 

    def convert(self: Vec[i16, 32]) -> Vec[u8, 32]:
        return Vec.__mm_cvt_epi162epi8(self)

    @llvm
    def __mm256_cvt_epi162epi8(x: Vec[i16, 16]) -> Vec[u8, 16]:
        %0 = trunc <16 x i16> %x to <16 x i8>
        ret <16 x i8> %0 

    def convert(self: Vec[i16, 16]) -> Vec[u8, 16]:
        return Vec.__mm256_cvt_epi162epi8(self)        

    @staticmethod
    @inline
    @llvm
    def _mm256_assignepi16(lst, index: int, index2: int, score, index3: int) -> None:
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        %0 = getelementptr <16 x i16>, ptr %lst, i64 %index, i64 %index2
        %1 = getelementptr i16, ptr %score, i64 %index3
        call void @llvm.prefetch(ptr %0, i32 0, i32 3, i32 1)
        call void @llvm.prefetch(ptr %1, i32 1, i32 3, i32 1)
        %2 = load i16, ptr %0
        store i16 %2, ptr %1
        ret {} {}  

    @staticmethod
    @inline    
    def assign(self: List[Vec[i16, 16]], other: int, other2: int, other3: List[i16], other4: int):
        return Vec._mm256_assignepi16(self.arr.ptr, other, other2, other3.arr.ptr, other4)

    @staticmethod
    @inline
    @llvm
    def _mm256_assignepf32(lst, index: int, index2: int, score, index3: int) -> None:
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        %0 = getelementptr <16 x f32>, ptr %lst, i64 %index, i64 %index2
        %1 = getelementptr f32, ptr %score, i64 %index3
        call void @llvm.prefetch(ptr %0, i32 0, i32 3, i32 1)
        call void @llvm.prefetch(ptr %1, i32 1, i32 3, i32 1)
        %2 = load f32, ptr %0
        store f32 %2, ptr %1
        ret {} {}  

    @staticmethod
    @inline    
    def assign_f32(self: List[Vec[f32, 16]], other: int, other2: int, other3: List[f32], other4: int):
        return Vec._mm256_assignepf32(self.arr.ptr, other, other2, other3.arr.ptr, other4)

    @staticmethod
    @inline
    @llvm
    def _mm256_vec_assignepi16_am(lst, score, index3: int, ms: ptr, z: Vec[i16, 16]) -> None:
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        %0 = getelementptr <16 x i16>, ptr %lst, i64 %index3
        %1 = getelementptr <16 x i16>, ptr %ms, i64 %index3
        %2 = getelementptr <16 x i16>, ptr %score, i64 %index3
        call void @llvm.prefetch(ptr %0, i32 0, i32 3, i32 1)
        call void @llvm.prefetch(ptr %1, i32 1, i32 3, i32 1)
        call void @llvm.prefetch(ptr %2, i32 1, i32 3, i32 1)
        %3 = load <16 x i16>, ptr %0
        %4 = load <16 x i16>, ptr %1
        %5 = sub <16 x i16> %4, %3
        %6 = icmp sle <16 x i16> %5, %z
        %7 = zext <16 x i1> %6 to <16 x i16>
        %8 = xor <16 x i16> %7, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
        %9 = mul <16 x i16> %7, %3
        %10 = mul <16 x i16> %8, <i16 -32768, i16 -32768, i16 -32768, i16 -32768, i16 -32768, i16 -32768, i16 -32768, i16 -32768, i16 -32768, i16 -32768, i16 -32768, i16 -32768, i16 -32768, i16 -32768, i16 -32768, i16 -32768>
        %11 = add <16 x i16> %9, %10
        store <16 x i16> %11, ptr %2        
        ret {} {}  


    @staticmethod
    @inline    
    def vec_assign_am(self: List[Vec[i16, 16]], other3: List[Vec[i16,16]], other4: int, other5: List[Vec[i16, 16]], other6: Vec[i16, 16]):
        return Vec._mm256_vec_assignepi16_am(self.arr.ptr, other3.arr.ptr, other4, other5.arr.ptr, other6)    

    @staticmethod
    @inline
    @llvm
    def _mm512_vec_assignepf32_am(lst, score, index3: int, ms: ptr, z: Vec[f32, 16]) -> None:
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        %0 = getelementptr <16 x f32>, ptr %lst, i64 %index3
        %1 = getelementptr <16 x f32>, ptr %ms, i64 %index3
        %2 = getelementptr <16 x f32>, ptr %score, i64 %index3
        call void @llvm.prefetch(ptr %0, i32 0, i32 3, i32 1)
        call void @llvm.prefetch(ptr %1, i32 1, i32 3, i32 1)
        call void @llvm.prefetch(ptr %2, i32 1, i32 3, i32 1)
        %3 = load <16 x f32>, ptr %0
        %4 = load <16 x f32>, ptr %1
        %5 = sub <16 x f32> %4, %3
        %6 = icmp sle <16 x f32> %5, %z
        %7 = zext <16 x i1> %6 to <16 x f32>
        %8 = xor <16 x f32> %7, <f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1, f32 1>
        %9 = mul <16 x f32> %7, %3
        %10 = mul <16 x f32> %8, <f32 -32768, f32 -32768, f32 -32768, f32 -32768, f32 -32768, f32 -32768, f32 -32768, f32 -32768, f32 -32768, f32 -32768, f32 -32768, f32 -32768, f32 -32768, f32 -32768, f32 -32768, f32 -32768>
        %11 = add <16 x f32> %9, %10
        store <16 x f32> %11, ptr %2        
        ret {} {}  


    @staticmethod
    @inline    
    def vec_assign_am_f32(self: List[Vec[f32, 16]], other3: List[Vec[f32,16]], other4: int, other5: List[Vec[f32, 16]], other6: Vec[f32, 16]):
        return Vec._mm512_vec_assignepf32_am(self.arr.ptr, other3.arr.ptr, other4, other5.arr.ptr, other6)    

    @staticmethod
    @inline
    @llvm
    def _mm256_vec_assignepi16(lst, score, index3: int) -> None:
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        %0 = getelementptr <16 x i16>, ptr %lst, i64 %index3
        %1 = getelementptr <16 x i16>, ptr %score, i64 %index3
        call void @llvm.prefetch(ptr %0, i32 0, i32 3, i32 1)
        call void @llvm.prefetch(ptr %1, i32 1, i32 3, i32 1)
        %2 = load <16 x i16>, ptr %0
        store <16 x i16> %2, ptr %1
        ret {} {}  

    @staticmethod
    @inline    
    def vec_assign(self: List[Vec[i16, 16]], other3: List[Vec[i16, 16]], other4: int):
        return Vec._mm256_vec_assignepi16(self.arr.ptr, other3.arr.ptr, other4)


    @staticmethod
    @inline
    @llvm
    def _mm256_assignepi16_am(lst, index: int, index2: int, score, index3: int, ms: ptr, z: i16) -> None:
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        %0 = getelementptr <16 x i16>, ptr %lst, i64 %index, i64 %index2
        %1 = getelementptr <16 x i16>, ptr %ms, i64 %index, i64 %index2
        %2 = getelementptr i16, ptr %score, i64 %index3
        call void @llvm.prefetch(ptr %0, i32 0, i32 3, i32 1)
        call void @llvm.prefetch(ptr %1, i32 1, i32 3, i32 1)
        call void @llvm.prefetch(ptr %2, i32 1, i32 3, i32 1)
        %3 = load i16, ptr %0
        %4 = load i16, ptr %1
        %5 = sub i16 %4, %3
        %6 = icmp sle i16 %5, %z
        br i1 %6, label %bfalse, label %btrue
        bfalse:
        store i16 %3, ptr %2        
        ret {} {}  
        btrue:
        store i16 -32768, ptr %2
        ret {} {}  


    @staticmethod
    @inline    
    def assign_am(self: List[Vec[i16, 16]], other: int, other2: int, other3: List[i16], other4: int, other5: List[Vec[i16, 16]], other6: i16):
        return Vec._mm256_assignepi16_am(self.arr.ptr, other, other2, other3.arr.ptr, other4, other5.arr.ptr, other6)        

    @staticmethod
    @inline
    @llvm
    def _mm512_assignepf32_am(lst, index: int, index2: int, score, index3: int, ms: ptr, z: f32) -> None:
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        %0 = getelementptr <16 x f32>, ptr %lst, i64 %index, i64 %index2
        %1 = getelementptr <16 x f32>, ptr %ms, i64 %index, i64 %index2
        %2 = getelementptr f32, ptr %score, i64 %index3
        call void @llvm.prefetch(ptr %0, i32 0, i32 3, i32 1)
        call void @llvm.prefetch(ptr %1, i32 1, i32 3, i32 1)
        call void @llvm.prefetch(ptr %2, i32 1, i32 3, i32 1)
        %3 = load f32, ptr %0
        %4 = load f32, ptr %1
        %5 = sub f32 %4, %3
        %6 = icmp sle f32 %5, %z
        br i1 %6, label %bfalse, label %btrue
        bfalse:
        store f32 %3, ptr %2        
        ret {} {}  
        btrue:
        store f32 -32768, ptr %2
        ret {} {}  


    @staticmethod
    @inline    
    def assign_am_f32(self: List[Vec[f32, 16]], other: int, other2: int, other3: List[f32], other4: int, other5: List[Vec[f32, 16]], other6: f32):
        return Vec._mm512_assignepf32_am(self.arr.ptr, other, other2, other3.arr.ptr, other4, other5.arr.ptr, other6)        

    @staticmethod
    @inline
    @llvm
    def _mm512_assignepi32(lst, index: int, index2: int, score, index3: int) -> None:
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        %0 = getelementptr <32 x i16>, ptr %lst, i64 %index, i64 %index2
        %1 = getelementptr i16, ptr %score, i64 %index3
        call void @llvm.prefetch(ptr %0, i32 0, i32 3, i32 1)
        call void @llvm.prefetch(ptr %1, i32 1, i32 3, i32 1)
        %2 = load i16, ptr %0
        store i16 %2, ptr %1
        ret {} {}  

    @staticmethod
    @inline    
    def assign(self: List[Vec[i16, 32]], other: int, other2: int, other3: List[i16], other4: int):
        return Vec._mm512_assignepi32(self.arr.ptr, other, other2, other3.arr.ptr, other4)


    @staticmethod
    @inline
    @llvm
    def _mm512_assignepi32_am(lst, index: int, index2: int, score, index3: int, ms: ptr, z: i16) -> None:
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        %0 = getelementptr <32 x i16>, ptr %lst, i64 %index, i64 %index2
        %1 = getelementptr <32 x i16>, ptr %ms, i64 %index, i64 %index2
        %2 = getelementptr i16, ptr %score, i64 %index3
        call void @llvm.prefetch(ptr %0, i32 0, i32 3, i32 1)
        call void @llvm.prefetch(ptr %1, i32 1, i32 3, i32 1)
        call void @llvm.prefetch(ptr %2, i32 1, i32 3, i32 1)
        %3 = load i16, ptr %0
        %4 = load i16, ptr %1
        %5 = sub i16 %4, %3
        %6 = icmp sgt i16 %5, %z
        br i1 %6, label %btrue, label %bfalse
        bfalse:
        store i16 %3, ptr %2        
        br label %end
        btrue:
        store i16 -32768, ptr %2
        br label %end
        end:
        ret {} {}  

    @staticmethod
    @inline    
    def assign_am(self: List[Vec[i16, 32]], other: int, other2: int, other3: List[i16], other4: int, other5: List[Vec[i16, 32]], other6: i16):
        return Vec._mm512_assignepi32_am(self.arr.ptr, other, other2, other3.arr.ptr, other4, other5.arr.ptr, other6)   

    @staticmethod
    @inline
    @llvm
    def _mm512_assignepi8(lst, index: int, index2: int, score, index3: int) -> None:
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        %0 = getelementptr <8 x i16>, ptr %lst, i64 %index, i64 %index2
        %1 = getelementptr i16, ptr %score, i64 %index3
        call void @llvm.prefetch(ptr %0, i32 0, i32 3, i32 1)
        call void @llvm.prefetch(ptr %1, i32 1, i32 3, i32 1)
        %2 = load i16, ptr %0
        store i16 %2, ptr %1
        ret {} {}  

    @staticmethod
    @inline    
    def assign(self: List[Vec[i16, 8]], other: int, other2: int, other3: List[i16], other4: int):
        return Vec._mm512_assignepi8(self.arr.ptr, other, other2, other3.arr.ptr, other4)

    @staticmethod
    @inline
    @llvm
    def _mm512_assignepf32(lst, index: int, index2: int, score, index3: int) -> None:
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        %0 = getelementptr <8 x f32>, ptr %lst, i64 %index, i64 %index2
        %1 = getelementptr f32, ptr %score, i64 %index3
        call void @llvm.prefetch(ptr %0, i32 0, i32 3, i32 1)
        call void @llvm.prefetch(ptr %1, i32 1, i32 3, i32 1)
        %2 = load f32, ptr %0
        store f32 %2, ptr %1
        ret {} {}  

    @staticmethod
    @inline    
    def assign_f32(self: List[Vec[f32, 8]], other: int, other2: int, other3: List[f32], other4: int):
        return Vec._mm512_assignepf32(self.arr.ptr, other, other2, other3.arr.ptr, other4)

    @staticmethod
    @inline
    @llvm
    def _mm512_assignepi8_am(lst, index: int, index2: int, score, index3: int, ms: ptr, z: i16) -> None:
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        %0 = getelementptr <8 x i16>, ptr %lst, i64 %index, i64 %index2
        %1 = getelementptr <8 x i16>, ptr %ms, i64 %index, i64 %index2
        %2 = getelementptr i16, ptr %score, i64 %index3
        call void @llvm.prefetch(ptr %0, i32 0, i32 3, i32 1)
        call void @llvm.prefetch(ptr %1, i32 1, i32 3, i32 1)
        call void @llvm.prefetch(ptr %2, i32 1, i32 3, i32 1)
        %3 = load i16, ptr %0
        %4 = load i16, ptr %1
        %5 = sub i16 %4, %3
        %6 = icmp sgt i16 %5, %z
        br i1 %6, label %btrue, label %bfalse
        bfalse:
        store i16 %3, ptr %2        
        br label %end
        btrue:
        store i16 -32768, ptr %2
        br label %end
        end:
        ret {} {}  

    @staticmethod
    @inline    
    def assign_am(self: List[Vec[i16, 8]], other: int, other2: int, other3: List[i16], other4: int, other5: List[Vec[i16, 8]], other6: i16):
        return Vec._mm512_assignepi8_am(self.arr.ptr, other, other2, other3.arr.ptr, other4, other5.arr.ptr, other6)       

    @staticmethod
    @inline
    @llvm
    def _mm512_assignepi8_am_f32(lst, index: int, index2: int, score, index3: int, ms: ptr, z: f32) -> None:
        declare void @llvm.prefetch(ptr, i32, i32, i32)
        %0 = getelementptr <8 x f32>, ptr %lst, i64 %index, i64 %index2
        %1 = getelementptr <8 x f32>, ptr %ms, i64 %index, i64 %index2
        %2 = getelementptr f32, ptr %score, i64 %index3
        call void @llvm.prefetch(ptr %0, i32 0, i32 3, i32 1)
        call void @llvm.prefetch(ptr %1, i32 1, i32 3, i32 1)
        call void @llvm.prefetch(ptr %2, i32 1, i32 3, i32 1)
        %3 = load f32, ptr %0
        %4 = load f32, ptr %1
        %5 = sub f32 %4, %3
        %6 = icmp sgt f32 %5, %z
        br i1 %6, label %btrue, label %bfalse
        bfalse:
        store f32 %3, ptr %2        
        br label %end
        btrue:
        store f32 -32768, ptr %2
        br label %end
        end:
        ret {} {}  

    @staticmethod
    @inline    
    def assign_am_f32(self: List[Vec[f32, 8]], other: int, other2: int, other3: List[f32], other4: int, other5: List[Vec[f32, 8]], other6: f32):
        return Vec._mm512_assignepi8_am_f32(self.arr.ptr, other, other2, other3.arr.ptr, other4, other5.arr.ptr, other6)       

    def __repr__(self):
        if N == 8:
            return f"<{self[0]}, {self[1]}, {self[2]}, {self[3]}, {self[4]}, {self[5]}, {self[6]}, {self[7]}>"
        elif N == 16:
            return f"<{self[0]}, {self[1]}, {self[2]}, {self[3]}, {self[4]}, {self[5]}, {self[6]}, {self[7]}, {self[8]}, {self[9]}, {self[10]}, {self[11]}, {self[12]}, {self[13]}, {self[14]}, {self[15]}>"
        elif N == 4:
            return f"<{self[0]}, {self[1]}, {self[2]}, {self[3]}>"
        elif N == 32:
            return f"<{self[0]}, {self[1]}, {self[2]}, {self[3]}, {self[4]}, {self[5]}, {self[6]}, {self[7]}, {self[8]}, {self[9]}, {self[10]}, {self[11]}, {self[12]}, {self[13]}, {self[14]}, {self[15]}, {self[16]}, {self[17]}, {self[18]}, {self[19]}, {self[20]}, {self[21]}, {self[22]}, {self[23]}, {self[24]}, {self[25]}, {self[26]}, {self[27]}, {self[28]}, {self[29]}, {self[30]}, {self[31]}>"            
        elif N == 64:
            return f"<{self[0]}, {self[1]}, {self[2]}, {self[3]}, {self[4]}, {self[5]}, {self[6]}, {self[7]}, {self[8]}, {self[9]}, {self[10]}, {self[11]}, {self[12]}, {self[13]}, {self[14]}, {self[15]}, {self[16]}, {self[17]}, {self[18]}, {self[19]}, {self[20]}, {self[21]}, {self[22]}, {self[23]}, {self[24]}, {self[25]}, {self[26]}, {self[27]}, {self[28]}, {self[29]}, {self[30]}, {self[31]}, {self[32]}, {self[33]}, {self[34]}, {self[35]}, {self[36]}, {self[37]}, {self[38]}, {self[39]}, {self[40]}, {self[41]}, {self[42]}, {self[43]}, {self[44]}, {self[45]}, {self[46]}, {self[47]}, {self[48]}, {self[49]}, {self[50]}, {self[51]}, {self[52]}, {self[53]}, {self[54]}, {self[55]}, {self[56]}, {self[57]}, {self[58]}, {self[59]}, {self[60]}, {self[61]}, {self[62]}, {self[63]}>"
        else:
            return "?"

u8x16 = Vec[u8, 16]
u8x32 = Vec[u8, 32]
f32x8 = Vec[f32, 8]
i32x16 = Vec[i32, 16]
i32x4 = Vec[i32, 4]
